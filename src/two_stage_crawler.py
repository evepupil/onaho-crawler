#!/usr/bin/env python3
"""
ä¸¤é˜¶æ®µçˆ¬å–ç­–ç•¥ï¼ˆæ”¯æŒæ–­ç‚¹ç»­çˆ¬ï¼‰
é˜¶æ®µ1: å¿«é€Ÿé€’å½’çˆ¬å–æ‰€æœ‰é“¾æ¥ï¼ˆä¸ä½¿ç”¨LLMï¼ŒèŠ‚çœtokenï¼‰
é˜¶æ®µ2: æ ¹æ®URLæ¨¡å¼è¿‡æ»¤äº§å“é¡µï¼Œè¯¦ç»†çˆ¬å–å¹¶LLMåˆ†æ

ç‰¹æ€§ï¼š
- é“¾æ¥åˆ—è¡¨å¸¦çˆ¬å–çŠ¶æ€æ ‡è®°
- æ”¯æŒæ–­ç‚¹ç»­çˆ¬
- æ‰¹æ¬¡çˆ¬å–å¤§ç½‘ç«™
"""

import asyncio
import json
import re
from datetime import datetime
from pathlib import Path
from urllib.parse import urljoin, urlparse
from typing import List, Set, Dict, Optional
import logging

from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy

import sys
sys.path.insert(0, str(Path(__file__).parent.parent))
from configs.config import LLM_CONFIG, CRAWLER_CONFIG

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(message)s'
)
logger = logging.getLogger(__name__)


class TwoStageCrawler:
    """ä¸¤é˜¶æ®µçˆ¬å–å™¨ï¼ˆæ”¯æŒæ–­ç‚¹ç»­çˆ¬ï¼‰"""

    def __init__(
        self,
        task_name: str,
        start_url: str,
        template_path: str = None,
        output_dir: str = "output",
        llm_config_key: str = "deepseek"
    ):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            task_name: ä»»åŠ¡åç§°ï¼ˆç”¨äºåˆ›å»ºè¾“å‡ºç›®å½•ï¼‰
            start_url: èµ·å§‹URL
            template_path: æ¨¡æ¿æ–‡ä»¶è·¯å¾„ï¼ˆé˜¶æ®µ2ä½¿ç”¨ï¼‰
            output_dir: è¾“å‡ºæ ¹ç›®å½•
            llm_config_key: LLMé…ç½®é”®
        """
        self.task_name = task_name
        self.start_url = start_url
        self.template_path = template_path

        # ä»»åŠ¡è¾“å‡ºç›®å½•ï¼šoutput/ä»»åŠ¡åç§°/
        self.task_output_dir = Path(output_dir) / task_name
        self.task_output_dir.mkdir(parents=True, exist_ok=True)

        # LLMé…ç½®
        llm_cfg = LLM_CONFIG.get(llm_config_key, {})
        self.provider = llm_cfg.get("provider")
        self.api_token = llm_cfg.get("api_token")

        # åŠ è½½æ¨¡æ¿
        self.template = None
        if template_path:
            with open(template_path, 'r', encoding='utf-8') as f:
                self.template = json.load(f)

        # æ•°æ®å­˜å‚¨
        self.all_links: List[Dict] = []  # é“¾æ¥åˆ—è¡¨ï¼ˆå¸¦çŠ¶æ€ï¼‰
        self.visited_urls: Set[str] = set()  # å·²è®¿é—®çš„URL
        self.products = []  # æå–çš„äº§å“æ•°æ®

        # æ–‡ä»¶è·¯å¾„
        self.links_file = self.task_output_dir / "collected_links.json"
        self.products_file = self.task_output_dir / "products.json"
        self.stage1_flag = self.task_output_dir / ".stage1_completed"

        logger.info(f"ä»»åŠ¡è¾“å‡ºç›®å½•: {self.task_output_dir}")

    def _normalize_url(self, url: str) -> str:
        """æ ‡å‡†åŒ–URL"""
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}{parsed.path}"

    def _is_same_domain(self, url: str, base_url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒåŸŸå"""
        try:
            url_domain = urlparse(url).netloc
            base_domain = urlparse(base_url).netloc
            return url_domain == base_domain
        except:
            return False

    def is_stage1_completed(self) -> bool:
        """æ£€æŸ¥é˜¶æ®µ1æ˜¯å¦å·²å®Œæˆ"""
        return self.stage1_flag.exists() and self.links_file.exists()

    def load_links(self) -> List[Dict]:
        """ä»æ–‡ä»¶åŠ è½½é“¾æ¥åˆ—è¡¨"""
        if not self.links_file.exists():
            logger.warning(f"é“¾æ¥æ–‡ä»¶ä¸å­˜åœ¨: {self.links_file}")
            return []

        with open(self.links_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        links = data.get("links", [])
        logger.info(f"ä»æ–‡ä»¶åŠ è½½äº† {len(links)} ä¸ªé“¾æ¥")

        # ç»Ÿè®¡å·²çˆ¬å–æ•°é‡
        crawled_count = sum(1 for link in links if link.get("crawled", False))
        logger.info(f"å…¶ä¸­å·²çˆ¬å–: {crawled_count} ä¸ªï¼Œæœªçˆ¬å–: {len(links) - crawled_count} ä¸ª")

        return links

    def _save_links(self, mark_stage1_complete: bool = True):
        """
        ä¿å­˜é“¾æ¥åˆ—è¡¨

        Args:
            mark_stage1_complete: æ˜¯å¦æ ‡è®°é˜¶æ®µ1å®Œæˆ
        """
        data = {
            "task_name": self.task_name,
            "start_url": self.start_url,
            "collected_at": datetime.now().isoformat(),
            "total_links": len(self.all_links),
            "crawled_count": sum(1 for link in self.all_links if link.get("crawled", False)),
            "links": self.all_links
        }

        with open(self.links_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ’¾ é“¾æ¥å·²ä¿å­˜åˆ°: {self.links_file}")

        # æ ‡è®°é˜¶æ®µ1å®Œæˆ
        if mark_stage1_complete:
            self.stage1_flag.touch()
            logger.info("âœ… é˜¶æ®µ1å·²æ ‡è®°ä¸ºå®Œæˆ")

    async def stage1_collect_links(
        self,
        max_depth: int = 3,
        max_pages: int = 100,
        force: bool = False
    ) -> List[Dict]:
        """
        é˜¶æ®µ1: å¿«é€Ÿé€’å½’çˆ¬å–ï¼Œæ”¶é›†æ‰€æœ‰é“¾æ¥

        Args:
            max_depth: æœ€å¤§é€’å½’æ·±åº¦
            max_pages: æœ€å¤§çˆ¬å–é¡µé¢æ•°
            force: æ˜¯å¦å¼ºåˆ¶é‡æ–°çˆ¬å–ï¼ˆå³ä½¿é˜¶æ®µ1å·²å®Œæˆï¼‰

        Returns:
            æ”¶é›†åˆ°çš„æ‰€æœ‰é“¾æ¥ï¼ˆå¸¦çŠ¶æ€ï¼‰
        """
        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆé˜¶æ®µ1
        if not force and self.is_stage1_completed():
            logger.info("="*60)
            logger.info("âš ï¸  é˜¶æ®µ1å·²å®Œæˆï¼Œè·³è¿‡")
            logger.info(f"å¦‚éœ€é‡æ–°çˆ¬å–ï¼Œè¯·åˆ é™¤æ–‡ä»¶: {self.stage1_flag}")
            logger.info("æˆ–ä½¿ç”¨ force=True å‚æ•°")
            logger.info("="*60)
            self.all_links = self.load_links()
            return self.all_links

        logger.info("="*60)
        logger.info("é˜¶æ®µ1: æ”¶é›†æ‰€æœ‰é“¾æ¥ï¼ˆä¸ä½¿ç”¨LLMï¼‰")
        logger.info(f"ä»»åŠ¡åç§°: {self.task_name}")
        logger.info(f"èµ·å§‹URL: {self.start_url}")
        logger.info(f"æœ€å¤§æ·±åº¦: {max_depth}, æœ€å¤§é¡µé¢æ•°: {max_pages}")
        logger.info("="*60)

        await self._collect_links_recursive(
            self.start_url,
            current_depth=0,
            max_depth=max_depth,
            max_pages=max_pages
        )

        logger.info(f"é˜¶æ®µ1å®Œæˆ: è®¿é—®äº† {len(self.visited_urls)} ä¸ªé¡µé¢ï¼Œæ”¶é›†åˆ° {len(self.all_links)} ä¸ªé“¾æ¥")

        # ä¿å­˜æ”¶é›†åˆ°çš„é“¾æ¥å¹¶æ ‡è®°é˜¶æ®µ1å®Œæˆ
        self._save_links(mark_stage1_complete=True)

        return self.all_links

    async def _collect_links_recursive(
        self,
        url: str,
        current_depth: int,
        max_depth: int,
        max_pages: int
    ):
        """é€’å½’æ”¶é›†é“¾æ¥"""
        if current_depth > max_depth or len(self.visited_urls) >= max_pages:
            return

        normalized_url = self._normalize_url(url)

        if normalized_url in self.visited_urls:
            return

        self.visited_urls.add(normalized_url)
        logger.info(f"[æ·±åº¦{current_depth}] è®¿é—®: {normalized_url}")

        # çˆ¬å–é¡µé¢ï¼ˆä¸ä½¿ç”¨LLMï¼‰
        browser_config = BrowserConfig(headless=True, verbose=False)
        run_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)

        try:
            async with AsyncWebCrawler(config=browser_config) as crawler:
                result = await crawler.arun(url=normalized_url, config=run_config)

                if result.success and hasattr(result, 'links') and result.links:
                    internal_links = result.links.get('internal', [])

                    # æå–é“¾æ¥
                    new_links = []
                    for link in internal_links:
                        if isinstance(link, dict):
                            link_url = link.get('href', '')
                        else:
                            link_url = str(link)

                        if link_url:
                            full_url = urljoin(url, link_url)
                            normalized = self._normalize_url(full_url)

                            # åªæ”¶é›†åŒåŸŸåçš„é“¾æ¥
                            if self._is_same_domain(normalized, self.start_url):
                                # æ£€æŸ¥æ˜¯å¦å·²æ·»åŠ 
                                if not any(l['url'] == normalized for l in self.all_links):
                                    self.all_links.append({
                                        "url": normalized,
                                        "crawled": False,
                                        "discovered_at": datetime.now().isoformat(),
                                        "depth": current_depth + 1
                                    })

                                if normalized not in self.visited_urls:
                                    new_links.append(normalized)

                    # å»¶è¿Ÿ
                    await asyncio.sleep(0.5)

                    # é€’å½’çˆ¬å–ï¼ˆé™åˆ¶æ•°é‡ï¼‰
                    if current_depth < max_depth:
                        for link_url in new_links[:5]:  # æ¯é¡µæœ€å¤šçˆ¬5ä¸ªå­é“¾æ¥
                            if len(self.visited_urls) >= max_pages:
                                break
                            await self._collect_links_recursive(
                                link_url,
                                current_depth + 1,
                                max_depth,
                                max_pages
                            )
                else:
                    logger.warning(f"çˆ¬å–å¤±è´¥æˆ–æ— é“¾æ¥: {normalized_url}")

        except Exception as e:
            logger.error(f"çˆ¬å–å¼‚å¸¸: {normalized_url} - {e}")

    def filter_product_links(
        self,
        url_patterns: List[str] = None,
        only_uncrawled: bool = True
    ) -> List[Dict]:
        """
        è¿‡æ»¤å‡ºäº§å“é“¾æ¥

        Args:
            url_patterns: URLåŒ¹é…æ¨¡å¼åˆ—è¡¨ï¼Œæ”¯æŒä¸¤ç§æ ¼å¼ï¼š
                         1. å­—ç¬¦ä¸²åŒ…å«åŒ¹é…ï¼šå¦‚ "/product/", "/item/"
                         2. æ­£åˆ™è¡¨è¾¾å¼ï¼ˆä»¥regex:å¼€å¤´ï¼‰ï¼šå¦‚ "regex:/\\d+\\.html$"
            only_uncrawled: æ˜¯å¦åªè¿”å›æœªçˆ¬å–çš„é“¾æ¥

        Returns:
            è¿‡æ»¤åçš„äº§å“é“¾æ¥åˆ—è¡¨

        ç¤ºä¾‹:
            # åŒ¹é… /product/ è·¯å¾„
            filter_product_links(["/product/", "/item/"])

            # åŒ¹é…æ•°å­—ID.htmlæ ¼å¼ï¼ˆå¦‚ /34435.htmlï¼‰
            filter_product_links(["regex:/\\d+\\.html$"])

            # æ··åˆä½¿ç”¨
            filter_product_links(["/product/", "regex:/\\d+\\.html$"])
        """
        # ä»æ–‡ä»¶åŠ è½½ï¼ˆå¦‚æœall_linksä¸ºç©ºï¼‰
        if not self.all_links:
            self.all_links = self.load_links()

        if not self.all_links:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„é“¾æ¥")
            return []

        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        compiled_patterns = []
        string_patterns = []

        if url_patterns:
            for pattern in url_patterns:
                if pattern.startswith("regex:"):
                    # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
                    regex_str = pattern[6:]  # ç§»é™¤ "regex:" å‰ç¼€
                    try:
                        compiled_patterns.append(re.compile(regex_str))
                        logger.info(f"  ä½¿ç”¨æ­£åˆ™: {regex_str}")
                    except re.error as e:
                        logger.warning(f"  æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯: {regex_str} - {e}")
                else:
                    # å­—ç¬¦ä¸²åŒ…å«æ¨¡å¼
                    string_patterns.append(pattern)
                    logger.info(f"  ä½¿ç”¨å­—ç¬¦ä¸²åŒ¹é…: {pattern}")

        # åº”ç”¨URLæ¨¡å¼è¿‡æ»¤
        filtered_links = []
        for link in self.all_links:
            url = link['url']

            # URLæ¨¡å¼åŒ¹é…
            if url_patterns:
                matched = False

                # æ£€æŸ¥å­—ç¬¦ä¸²åŒ…å«åŒ¹é…
                if string_patterns and any(pattern in url for pattern in string_patterns):
                    matched = True

                # æ£€æŸ¥æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
                if compiled_patterns and any(pattern.search(url) for pattern in compiled_patterns):
                    matched = True

                if not matched:
                    continue

            # çˆ¬å–çŠ¶æ€è¿‡æ»¤
            if only_uncrawled and link.get('crawled', False):
                continue

            filtered_links.append(link)

        # ç»Ÿè®¡ä¿¡æ¯
        total_matched = 0
        if url_patterns:
            for link in self.all_links:
                url = link['url']
                if string_patterns and any(p in url for p in string_patterns):
                    total_matched += 1
                elif compiled_patterns and any(p.search(url) for p in compiled_patterns):
                    total_matched += 1

        crawled_count = sum(1 for l in self.all_links if l.get('crawled', False))

        logger.info(f"é“¾æ¥è¿‡æ»¤ç»“æœ:")
        logger.info(f"  æ€»é“¾æ¥æ•°: {len(self.all_links)}")
        if url_patterns:
            logger.info(f"  åŒ¹é…æ¨¡å¼çš„: {total_matched}")
        logger.info(f"  å·²çˆ¬å–: {crawled_count}")
        logger.info(f"  å¾…çˆ¬å–ï¼ˆæœ¬æ¬¡ï¼‰: {len(filtered_links)}")

        return filtered_links

    def _create_extraction_instruction(self) -> str:
        """åˆ›å»ºLLMæå–æŒ‡ä»¤"""
        if not self.template:
            raise ValueError("æœªè®¾ç½®æ¨¡æ¿")

        template_str = json.dumps(self.template, ensure_ascii=False, indent=2)

        instruction = f"""
ä»ç½‘é¡µå†…å®¹ä¸­æå–äº§å“ä¿¡æ¯ï¼Œä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ã€‚

æ¨¡æ¿å®šä¹‰ï¼ˆé”®æ˜¯å­—æ®µåï¼Œå€¼æ˜¯å­—æ®µè¯´æ˜ï¼‰ï¼š
{template_str}

**é‡è¦è¦æ±‚**ï¼š
1. ç›´æ¥è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼Œä¸è¦è¿”å›æ•°ç»„æˆ–å…¶ä»–æ ¼å¼
2. JSONå¯¹è±¡çš„é”®å¿…é¡»å®Œå…¨åŒ¹é…æ¨¡æ¿ä¸­çš„é”®å
3. æ¯ä¸ªé”®çš„å€¼æ˜¯ä»ç½‘é¡µä¸­æå–çš„å®é™…æ•°æ®
4. å¦‚æœæŸä¸ªå­—æ®µåœ¨ç½‘é¡µä¸­æ‰¾ä¸åˆ°ï¼Œè®¾ç½®ä¸º null
5. ä¸è¦æ·»åŠ  index, tags, content, error ç­‰é¢å¤–å­—æ®µ
6. ä¸è¦ä½¿ç”¨ blocks æ ¼å¼
7. åªè¿”å›çº¯JSONå¯¹è±¡ï¼Œä¸è¦ä»»ä½•å…¶ä»–å†…å®¹

å†æ¬¡å¼ºè°ƒï¼šç›´æ¥è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«æ¨¡æ¿ä¸­å®šä¹‰çš„æ‰€æœ‰å­—æ®µã€‚
"""
        return instruction

    def _mark_link_crawled(self, url: str):
        """æ ‡è®°é“¾æ¥ä¸ºå·²çˆ¬å–"""
        for link in self.all_links:
            if link['url'] == url:
                link['crawled'] = True
                link['crawled_at'] = datetime.now().isoformat()
                break

        # æ›´æ–°æ–‡ä»¶
        self._save_links(mark_stage1_complete=False)

    async def stage2_extract_products(
        self,
        product_links: List[Dict] = None,
        url_patterns: List[str] = None,
        batch_size: int = None,
        save_interval: int = 5
    ) -> List[Dict]:
        """
        é˜¶æ®µ2: è¯¦ç»†çˆ¬å–äº§å“é¡µå¹¶ç”¨LLMæå–æ•°æ®

        Args:
            product_links: äº§å“é“¾æ¥åˆ—è¡¨ï¼ˆå¦‚æœä¸ºNoneï¼Œè‡ªåŠ¨æ ¹æ®url_patternsè¿‡æ»¤ï¼‰
            url_patterns: URLåŒ¹é…æ¨¡å¼ï¼ˆå¦‚æœproduct_linksä¸ºNoneæ—¶ä½¿ç”¨ï¼‰
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆæ¯æ¬¡çˆ¬å–å¤šå°‘ä¸ªé“¾æ¥ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
            save_interval: æ¯çˆ¬å–å¤šå°‘ä¸ªä¿å­˜ä¸€æ¬¡

        Returns:
            æå–çš„äº§å“æ•°æ®åˆ—è¡¨
        """
        logger.info("="*60)
        logger.info("é˜¶æ®µ2: è¯¦ç»†çˆ¬å–äº§å“é¡µå¹¶LLMåˆ†æ")
        logger.info(f"ä»»åŠ¡åç§°: {self.task_name}")
        logger.info("="*60)

        # è‡ªåŠ¨è¿‡æ»¤äº§å“é“¾æ¥
        if product_links is None:
            if url_patterns is None:
                raise ValueError("å¿…é¡»æä¾› product_links æˆ– url_patterns")
            product_links = self.filter_product_links(url_patterns=url_patterns, only_uncrawled=True)

        if not product_links:
            logger.warning("æ²¡æœ‰å¾…çˆ¬å–çš„äº§å“é“¾æ¥")
            return self.products

        # æ‰¹æ¬¡é™åˆ¶
        if batch_size:
            product_links = product_links[:batch_size]
            logger.info(f"æ‰¹æ¬¡å¤§å°é™åˆ¶: {batch_size} ä¸ªé“¾æ¥")

        logger.info(f"å¾…çˆ¬å–äº§å“æ•°: {len(product_links)}")

        if not self.template:
            raise ValueError("æœªè®¾ç½®æ¨¡æ¿ï¼Œæ— æ³•è¿›è¡ŒLLMæå–")

        # åŠ è½½å·²æœ‰äº§å“æ•°æ®ï¼ˆæ–­ç‚¹ç»­çˆ¬ï¼‰
        if self.products_file.exists():
            with open(self.products_file, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
                self.products = existing_data.get("products", [])
            logger.info(f"å·²åŠ è½½ {len(self.products)} ä¸ªå·²æå–çš„äº§å“")

        # åˆ›å»ºLLMæå–ç­–ç•¥
        llm_config = LLMConfig(
            provider=self.provider,
            api_token=self.api_token
        )

        extraction_strategy = LLMExtractionStrategy(
            llm_config=llm_config,
            instruction=self._create_extraction_instruction(),
            verbose=True
        )

        browser_config = BrowserConfig(headless=True, verbose=False)
        run_config = CrawlerRunConfig(
            extraction_strategy=extraction_strategy,
            cache_mode=CacheMode.BYPASS
        )

        async with AsyncWebCrawler(config=browser_config) as crawler:
            for i, link_info in enumerate(product_links, 1):
                url = link_info['url']
                logger.info(f"[{i}/{len(product_links)}] çˆ¬å–: {url}")

                try:
                    result = await crawler.arun(url=url, config=run_config)

                    if result.success and result.extracted_content:
                        extracted = json.loads(result.extracted_content)

                        # å¤„ç†è¿”å›æ ¼å¼
                        if isinstance(extracted, list) and len(extracted) > 0:
                            data = extracted[0]
                        elif isinstance(extracted, dict):
                            data = extracted
                        else:
                            logger.warning(f"æ„å¤–çš„è¿”å›æ ¼å¼: {type(extracted)}")
                            self._mark_link_crawled(url)
                            continue

                        # æ£€æŸ¥æ˜¯å¦æ˜¯blocksæ ¼å¼
                        if 'index' in data and 'content' in data:
                            logger.warning(f"è¿”å›äº†blocksæ ¼å¼ï¼Œè·³è¿‡")
                            self._mark_link_crawled(url)
                            continue

                        # æ·»åŠ å…ƒæ•°æ®
                        data['_source_url'] = url
                        data['_crawled_at'] = datetime.now().isoformat()

                        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
                        has_data = any(
                            v is not None and str(v).strip() != ""
                            for k, v in data.items()
                            if not k.startswith('_')
                        )

                        if has_data:
                            self.products.append(data)
                            logger.info(f"âœ… æˆåŠŸæå–æ•°æ®")
                        else:
                            logger.warning(f"æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®")

                        # æ ‡è®°ä¸ºå·²çˆ¬å–
                        self._mark_link_crawled(url)

                    else:
                        logger.error(f"çˆ¬å–å¤±è´¥: {result.error_message if not result.success else 'æ— æå–å†…å®¹'}")
                        # å³ä½¿å¤±è´¥ä¹Ÿæ ‡è®°ä¸ºå·²çˆ¬å–ï¼Œé¿å…é‡å¤
                        self._mark_link_crawled(url)

                except Exception as e:
                    logger.error(f"å¤„ç†å¼‚å¸¸: {url} - {e}")
                    # å³ä½¿å¼‚å¸¸ä¹Ÿæ ‡è®°ä¸ºå·²çˆ¬å–
                    self._mark_link_crawled(url)

                # å®šæœŸä¿å­˜
                if i % save_interval == 0:
                    self._save_products()
                    logger.info(f"ğŸ’¾ å·²ä¿å­˜è¿›åº¦ ({i}/{len(product_links)})")

                # å»¶è¿Ÿ
                await asyncio.sleep(1)

        # æœ€ç»ˆä¿å­˜
        self._save_products()

        logger.info(f"é˜¶æ®µ2å®Œæˆ: æˆåŠŸæå– {len(self.products)} ä¸ªäº§å“")
        return self.products

    def _save_products(self):
        """ä¿å­˜äº§å“æ•°æ®"""
        if not self.products:
            return

        output = {
            "task_name": self.task_name,
            "template": self.template,
            "crawl_info": {
                "start_url": self.start_url,
                "total_links_collected": len(self.all_links),
                "products_extracted": len(self.products),
                "last_updated": datetime.now().isoformat()
            },
            "products": self.products
        }

        with open(self.products_file, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ’¾ äº§å“æ•°æ®å·²ä¿å­˜åˆ°: {self.products_file}")

    def save_products(self) -> Path:
        """ä¿å­˜äº§å“æ•°æ®ï¼ˆå¯¹å¤–æ¥å£ï¼‰"""
        self._save_products()
        return self.products_file

    def print_summary(self):
        """æ‰“å°æ‘˜è¦"""
        # ç»Ÿè®¡ä¿¡æ¯
        total_links = len(self.all_links)
        crawled_links = sum(1 for link in self.all_links if link.get('crawled', False))
        uncrawled_links = total_links - crawled_links

        print("\n" + "="*60)
        print("çˆ¬å–æ‘˜è¦")
        print("="*60)
        print(f"ä»»åŠ¡åç§°: {self.task_name}")
        print(f"èµ·å§‹URL: {self.start_url}")
        print(f"è¾“å‡ºç›®å½•: {self.task_output_dir}")
        print()
        print(f"é“¾æ¥ç»Ÿè®¡:")
        print(f"  æ”¶é›†æ€»æ•°: {total_links}")
        print(f"  å·²çˆ¬å–: {crawled_links}")
        print(f"  æœªçˆ¬å–: {uncrawled_links}")
        print()
        print(f"äº§å“ç»Ÿè®¡:")
        print(f"  æå–äº§å“æ•°: {len(self.products)}")

        if self.products:
            print(f"\næœ€è¿‘æå–çš„äº§å“:")
            for i, product in enumerate(self.products[-3:], 1):
                print(f"\nã€äº§å“ {i}ã€‘")
                print(f"æ¥æº: {product.get('_source_url', 'Unknown')}")
                for key, value in product.items():
                    if not key.startswith('_'):
                        print(f"  {key}: {value}")

            if len(self.products) > 3:
                print(f"\n... è¿˜æœ‰ {len(self.products) - 3} ä¸ªäº§å“")

        print("\næ–‡ä»¶ä½ç½®:")
        print(f"  é“¾æ¥æ–‡ä»¶: {self.links_file}")
        print(f"  äº§å“æ–‡ä»¶: {self.products_file}")
        print("="*60)

    async def run(
        self,
        url_patterns: List[str],
        stage1_max_depth: int = 3,
        stage1_max_pages: int = 100,
        stage2_batch_size: int = None,
        force_stage1: bool = False
    ):
        """
        å®Œæ•´è¿è¡Œä¸¤é˜¶æ®µçˆ¬å–ï¼ˆè‡ªåŠ¨æ–­ç‚¹ç»­çˆ¬ï¼‰

        Args:
            url_patterns: URLåŒ¹é…æ¨¡å¼
            stage1_max_depth: é˜¶æ®µ1æœ€å¤§æ·±åº¦
            stage1_max_pages: é˜¶æ®µ1æœ€å¤§é¡µé¢æ•°
            stage2_batch_size: é˜¶æ®µ2æ‰¹æ¬¡å¤§å°
            force_stage1: æ˜¯å¦å¼ºåˆ¶é‡æ–°æ‰§è¡Œé˜¶æ®µ1
        """
        # é˜¶æ®µ1: æ”¶é›†é“¾æ¥ï¼ˆå¦‚æœæœªå®Œæˆï¼‰
        await self.stage1_collect_links(
            max_depth=stage1_max_depth,
            max_pages=stage1_max_pages,
            force=force_stage1
        )

        # é˜¶æ®µ2: æå–äº§å“
        await self.stage2_extract_products(
            url_patterns=url_patterns,
            batch_size=stage2_batch_size
        )

        # æ‰“å°æ‘˜è¦
        self.print_summary()


async def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºä¸¤é˜¶æ®µçˆ¬å–ï¼ˆæ”¯æŒæ–­ç‚¹ç»­çˆ¬ï¼‰"""
    print("\nğŸ¤– ä¸¤é˜¶æ®µæ™ºèƒ½çˆ¬è™«ï¼ˆæ”¯æŒæ–­ç‚¹ç»­çˆ¬ï¼‰")
    print("="*60)

    # åˆ›å»ºçˆ¬è™«
    crawler = TwoStageCrawler(
        task_name="deepseek_pricing",  # ä»»åŠ¡åç§°
        start_url="https://api-docs.deepseek.com/zh-cn/",
        template_path="templates/template_deepseek_pricing.json",
        llm_config_key="deepseek"
    )

    # ä¸€é”®è¿è¡Œï¼ˆè‡ªåŠ¨æ£€æµ‹æ–­ç‚¹ï¼‰
    await crawler.run(
        url_patterns=["/pricing", "/quick_start"],
        stage1_max_depth=3,
        stage1_max_pages=50,
        stage2_batch_size=10  # æ¯æ¬¡æœ€å¤šçˆ¬10ä¸ªäº§å“é¡µ
    )

    print("\nğŸ‰ å®Œæˆï¼\n")


def load_tasks_from_config(config_path: str) -> List[Dict]:
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½ä»»åŠ¡åˆ—è¡¨

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        ä»»åŠ¡é…ç½®åˆ—è¡¨
    """
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)

    return config.get("tasks", [])


async def run_from_config(config_path: str, task_id: str = None):
    """
    ä»é…ç½®æ–‡ä»¶è¿è¡Œä»»åŠ¡

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        task_id: æŒ‡å®šä»»åŠ¡IDï¼Œä¸ºNoneåˆ™è¿è¡Œæ‰€æœ‰ä»»åŠ¡
    """
    print("\nğŸ¤– ä¸¤é˜¶æ®µæ™ºèƒ½çˆ¬è™«ï¼ˆä»é…ç½®æ–‡ä»¶åŠ è½½ï¼‰")
    print("="*60)
    print(f"é…ç½®æ–‡ä»¶: {config_path}")
    print("="*60)

    # åŠ è½½ä»»åŠ¡é…ç½®
    tasks = load_tasks_from_config(config_path)

    if not tasks:
        logger.error("é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰ä»»åŠ¡")
        return

    # è¿‡æ»¤æŒ‡å®šä»»åŠ¡
    if task_id:
        tasks = [t for t in tasks if t.get("task_id") == task_id]
        if not tasks:
            logger.error(f"æœªæ‰¾åˆ°ä»»åŠ¡: {task_id}")
            return

    logger.info(f"å¾…è¿è¡Œä»»åŠ¡æ•°: {len(tasks)}")

    # è¿è¡Œæ¯ä¸ªä»»åŠ¡
    for i, task_config in enumerate(tasks, 1):
        tid = task_config.get("task_id", f"task_{i}")
        task_name = task_config.get("task_name", tid)
        start_url = task_config.get("start_url")
        template_path = task_config.get("template_path")

        # é˜¶æ®µ1é…ç½®
        stage1 = task_config.get("stage1", {})
        max_depth = stage1.get("max_depth", 3)
        max_pages = stage1.get("max_pages", 100)

        # é˜¶æ®µ2é…ç½®
        stage2 = task_config.get("stage2", {})
        url_patterns = stage2.get("url_patterns", [])
        batch_size = stage2.get("batch_size", None)

        print(f"\n[{i}/{len(tasks)}] è¿è¡Œä»»åŠ¡: {task_name}")
        print(f"  èµ·å§‹URL: {start_url}")
        print(f"  æ¨¡æ¿: {template_path}")
        print(f"  é˜¶æ®µ1: æ·±åº¦={max_depth}, æœ€å¤§é¡µé¢={max_pages}")
        print(f"  é˜¶æ®µ2: æ¨¡å¼={url_patterns}, æ‰¹æ¬¡={batch_size}")

        # åˆ›å»ºçˆ¬è™«
        crawler = TwoStageCrawler(
            task_name=task_name,
            start_url=start_url,
            template_path=template_path,
            llm_config_key="deepseek"
        )

        # è¿è¡Œ
        await crawler.run(
            url_patterns=url_patterns,
            stage1_max_depth=max_depth,
            stage1_max_pages=max_pages,
            stage2_batch_size=batch_size
        )

        print(f"\nâœ… ä»»åŠ¡ {task_name} å®Œæˆ")

    print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼\n")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='ä¸¤é˜¶æ®µæ™ºèƒ½çˆ¬è™«')
    parser.add_argument('--config', '-f', nargs='?', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--task', '-t', help='æŒ‡å®šä»»åŠ¡ID')

    args = parser.parse_args()

    try:
        if args.config:
            # ä»é…ç½®æ–‡ä»¶è¿è¡Œ
            asyncio.run(run_from_config(args.config, args.task))
        else:
            # è¿è¡Œé»˜è®¤æ¼”ç¤º
            asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"ğŸš¨ ç¨‹åºå¼‚å¸¸: {e}", exc_info=True)

