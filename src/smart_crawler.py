#!/usr/bin/env python3
"""
äº§å“ä¿¡æ¯çˆ¬è™« - åŸºäºæ¨¡æ¿æå–ç»“æ„åŒ–æ•°æ®
æ ¹æ®crawl4aiå®˜æ–¹æ–‡æ¡£å®ç°

ä½¿ç”¨æ–¹æ³•:
    python smart_crawler.py

éœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡:
    export OPENAI_API_KEY='your-key'
    æˆ–
    export DEEPSEEK_API_KEY='your-key'
"""

import asyncio
import json
import os
from datetime import datetime
from pathlib import Path
from urllib.parse import urljoin, urlparse

from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy

# å¯¼å…¥é…ç½®
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))
from configs.config import LLM_CONFIG, CRAWLER_CONFIG

import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å¯ç”¨litellmè°ƒè¯•
try:
    import litellm
    # ä½¿ç”¨æ–°çš„ç¯å¢ƒå˜é‡æ–¹å¼
    os.environ['LITELLM_LOG'] = 'DEBUG'
    litellm.set_verbose = True
    logger.info("å·²å¯ç”¨ litellm è¯¦ç»†æ—¥å¿—")
except:
    pass


class SmartCrawler:
    """æ™ºèƒ½äº§å“ä¿¡æ¯çˆ¬è™« - ä½¿ç”¨LLMæå–ç»“æ„åŒ–æ•°æ®"""

    def __init__(
        self,
        template_path: str,
        output_dir: str = None,
        provider: str = None,
        api_token: str = None,
        llm_config_key: str = "deepseek"  # é»˜è®¤ä½¿ç”¨deepseeké…ç½®
    ):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            template_path: JSONæ¨¡æ¿æ–‡ä»¶è·¯å¾„ï¼ˆé”®å€¼å¯¹æ ¼å¼ï¼‰
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»configè¯»å–ï¼‰
            provider: LLM providerï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»configè¯»å–ï¼‰
            api_token: API tokenï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»configè¯»å–ï¼‰
            llm_config_key: config.pyä¸­LLM_CONFIGçš„é”®åï¼ˆé»˜è®¤"deepseek"ï¼‰
        """
        self.template_path = template_path

        # ä»configè¯»å–LLMé…ç½®
        llm_cfg = LLM_CONFIG.get(llm_config_key, {})

        self.output_dir = Path(output_dir or CRAWLER_CONFIG.get("output_dir", "output"))
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # åŠ è½½æ¨¡æ¿
        with open(template_path, 'r', encoding='utf-8') as f:
            self.template = json.load(f)

        # LLMé…ç½® - ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å‚æ•°ï¼Œå¦åˆ™ä»configè¯»å–
        self.provider = provider or llm_cfg.get("provider", "openai/gpt-4o-mini")
        self.api_token = api_token or llm_cfg.get("api_token")

        logger.info(f"ä»config.pyè¯»å–é…ç½®: provider={self.provider}, api_token={'å·²è®¾ç½®' if self.api_token else 'æœªè®¾ç½®'}")

        # çˆ¬å–ç»“æœ
        self.visited_urls = set()
        self.products = []

    def _create_extraction_instruction(self) -> str:
        """æ ¹æ®æ¨¡æ¿åˆ›å»ºLLMæå–æŒ‡ä»¤"""
        template_str = json.dumps(self.template, ensure_ascii=False, indent=2)

        instruction = f"""
ä»ç½‘é¡µå†…å®¹ä¸­æå–äº§å“ä¿¡æ¯ï¼Œä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ã€‚

æ¨¡æ¿å®šä¹‰ï¼ˆé”®æ˜¯å­—æ®µåï¼Œå€¼æ˜¯å­—æ®µè¯´æ˜ï¼‰ï¼š
{template_str}

**é‡è¦è¦æ±‚**ï¼š
1. ç›´æ¥è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼Œä¸è¦è¿”å›æ•°ç»„æˆ–å…¶ä»–æ ¼å¼
2. JSONå¯¹è±¡çš„é”®å¿…é¡»å®Œå…¨åŒ¹é…æ¨¡æ¿ä¸­çš„é”®å
3. æ¯ä¸ªé”®çš„å€¼æ˜¯ä»ç½‘é¡µä¸­æå–çš„å®é™…æ•°æ®
4. å¦‚æœæŸä¸ªå­—æ®µåœ¨ç½‘é¡µä¸­æ‰¾ä¸åˆ°ï¼Œè®¾ç½®ä¸º null
5. ä¸è¦æ·»åŠ  index, tags, content, error ç­‰é¢å¤–å­—æ®µ
6. ä¸è¦ä½¿ç”¨ blocks æ ¼å¼
7. åªè¿”å›çº¯JSONå¯¹è±¡ï¼Œä¸è¦ä»»ä½•å…¶ä»–å†…å®¹

å¿…é¡»ä¸¥æ ¼éµå®ˆçš„è¿”å›æ ¼å¼ç¤ºä¾‹ï¼š
{{
{chr(10).join([f'  "{k}": "å®é™…æå–çš„å€¼"' for k in self.template.keys()])}
}}

å†æ¬¡å¼ºè°ƒï¼šç›´æ¥è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«æ¨¡æ¿ä¸­å®šä¹‰çš„æ‰€æœ‰å­—æ®µï¼Œä¸è¦ä»»ä½•åŒ…è£…æˆ–é¢å¤–ç»“æ„ã€‚
"""
        return instruction

    def _normalize_url(self, url: str) -> str:
        """æ ‡å‡†åŒ–URL"""
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}{parsed.path}"

    def _is_valid_url(self, url: str, base_domain: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ"""
        try:
            parsed = urlparse(url)
            return parsed.scheme in ['http', 'https'] and parsed.netloc == base_domain
        except:
            return False

    async def crawl_page(self, url: str, depth: int = 0):
        """
        çˆ¬å–å•ä¸ªé¡µé¢å¹¶ä½¿ç”¨LLMæå–æ•°æ®

        Args:
            url: ç›®æ ‡URL
            depth: å½“å‰é€’å½’æ·±åº¦

        Returns:
            æå–çš„æ•°æ®å­—å…¸æˆ–None
        """
        normalized_url = self._normalize_url(url)

        if normalized_url in self.visited_urls:
            return None

        self.visited_urls.add(normalized_url)
        logger.info(f"[æ·±åº¦{depth}] çˆ¬å–: {normalized_url}")

        # åˆ›å»ºLLMæå–ç­–ç•¥ - æ ¹æ®crawl4aiæœ€æ–°æ–‡æ¡£
        llm_config = LLMConfig(
            provider=self.provider,
            api_token=self.api_token
        )

        extraction_instruction = self._create_extraction_instruction()

        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        logger.info("="*60)
        logger.info("LLMè°ƒç”¨é…ç½®:")
        logger.info(f"  Provider: {self.provider}")
        logger.info(f"  API Token: {'å·²è®¾ç½®' if self.api_token else 'æœªè®¾ç½®'}")
        logger.info(f"  Tokenå‰ç¼€: {self.api_token[:10] if self.api_token else 'N/A'}...")
        logger.info(f"  æŒ‡ä»¤é•¿åº¦: {len(extraction_instruction)} å­—ç¬¦")
        logger.info(f"  æŒ‡ä»¤é¢„è§ˆ: {extraction_instruction[:200]}...")
        logger.info("="*60)

        extraction_strategy = LLMExtractionStrategy(
            llm_config=llm_config,
            instruction=extraction_instruction,
            verbose=True  # å¼€å¯verboseçœ‹æ›´å¤šè°ƒè¯•ä¿¡æ¯
        )

        # æµè§ˆå™¨é…ç½®
        browser_config = BrowserConfig(
            headless=True,
            verbose=False
        )

        # çˆ¬å–é…ç½®
        run_config = CrawlerRunConfig(
            extraction_strategy=extraction_strategy,
            cache_mode=CacheMode.BYPASS
        )

        try:
            async with AsyncWebCrawler(config=browser_config) as crawler:
                result = await crawler.arun(
                    url=normalized_url,
                    config=run_config
                )

                if result.success and result.extracted_content:
                    try:
                        # æ‰“å°åŸå§‹è¿”å›å†…å®¹ç”¨äºè°ƒè¯•
                        logger.info(f"LLMè¿”å›å†…å®¹: {result.extracted_content[:500]}")

                        # è§£æLLMè¿”å›çš„JSON
                        extracted = json.loads(result.extracted_content)

                        # æ£€æŸ¥è¿”å›æ ¼å¼å¹¶æå–æ­£ç¡®çš„æ•°æ®
                        if isinstance(extracted, list):
                            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯blocksæ ¼å¼
                            if len(extracted) > 0:
                                first_item = extracted[0] if isinstance(extracted[0], dict) else {}

                                # æ£€æŸ¥æ˜¯å¦æ˜¯blocksæ ¼å¼ï¼ˆæœ‰index/tags/contentå­—æ®µï¼‰
                                if 'index' in first_item and 'content' in first_item:
                                    logger.warning("âš ï¸ LLMè¿”å›äº†blocksæ ¼å¼ï¼Œè€Œä¸æ˜¯æ¨¡æ¿æ ¼å¼ï¼Œè·³è¿‡æ­¤ç»“æœ")
                                    return None

                                # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯ä¿¡æ¯
                                if first_item.get('error') is True or 'error' in first_item.get('tags', []):
                                    error_msg = first_item.get('content', 'æœªçŸ¥é”™è¯¯')
                                    logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {error_msg}")
                                    return None

                                data = first_item
                            else:
                                data = {}
                        elif isinstance(extracted, dict):
                            # æ£€æŸ¥æ˜¯å¦æ˜¯blocksæ ¼å¼
                            if 'index' in extracted and 'content' in extracted:
                                logger.warning("âš ï¸ LLMè¿”å›äº†blocksæ ¼å¼ï¼Œè€Œä¸æ˜¯æ¨¡æ¿æ ¼å¼ï¼Œè·³è¿‡æ­¤ç»“æœ")
                                return None

                            # æ£€æŸ¥å­—å…¸æ˜¯å¦åŒ…å«é”™è¯¯
                            if extracted.get('error') is True:
                                error_msg = extracted.get('content', extracted.get('message', 'æœªçŸ¥é”™è¯¯'))
                                logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {error_msg}")
                                return None
                            data = extracted
                        else:
                            logger.error(f"âŒ æ„å¤–çš„è¿”å›æ ¼å¼: {type(extracted)}")
                            return None

                        # éªŒè¯è¿”å›çš„æ•°æ®æ˜¯å¦åŒ…å«æ¨¡æ¿å­—æ®µ
                        template_keys = set(self.template.keys())
                        data_keys = set(k for k in data.keys() if not k.startswith('_'))

                        # å¦‚æœè¿”å›çš„å­—æ®µå’Œæ¨¡æ¿å­—æ®µå®Œå…¨ä¸åŒ¹é…ï¼Œè¯´æ˜æ ¼å¼ä¸å¯¹
                        if not template_keys.intersection(data_keys):
                            logger.warning(f"âš ï¸ è¿”å›çš„å­—æ®µä¸æ¨¡æ¿ä¸åŒ¹é…ã€‚æ¨¡æ¿å­—æ®µ: {template_keys}, è¿”å›å­—æ®µ: {data_keys}")
                            return None

                        # æ·»åŠ å…ƒæ•°æ®
                        data['_source_url'] = normalized_url
                        data['_crawled_at'] = datetime.now().isoformat()

                        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
                        has_valid_data = any(
                            v is not None and str(v).strip() != ""
                            for k, v in data.items()
                            if not k.startswith('_')
                        )

                        if has_valid_data:
                            self.products.append(data)
                            logger.info(f"âœ… æˆåŠŸæå–æ•°æ®")
                        else:
                            logger.warning(f"âš ï¸  æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®")

                        # æ— è®ºæ˜¯å¦æ‰¾åˆ°æœ‰æ•ˆæ•°æ®ï¼Œéƒ½è¿”å›resultä»¥ä¾¿ç»§ç»­é€’å½’
                        return {
                            'data': data if has_valid_data else None,
                            'result': result,
                            'url': normalized_url
                        }

                    except json.JSONDecodeError as e:
                        logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
                        logger.error(f"è¿”å›å†…å®¹: {result.extracted_content[:500]}")
                        return None
                    except Exception as e:
                        logger.error(f"âŒ å¤„ç†æ•°æ®å¼‚å¸¸: {e}")
                        logger.error(f"è¿”å›å†…å®¹: {result.extracted_content[:500]}")
                        return None
                else:
                    logger.error(f"âŒ çˆ¬å–å¤±è´¥: {result.error_message if not result.success else 'æ— æå–å†…å®¹'}")
                    return None

        except Exception as e:
            logger.error(f"ğŸš¨ å¼‚å¸¸: {e}")
            return None

    async def crawl_recursive(
        self,
        start_url: str,
        max_depth: int = 2,
        max_pages: int = 10
    ):
        """
        é€’å½’çˆ¬å–

        Args:
            start_url: èµ·å§‹URL
            max_depth: æœ€å¤§é€’å½’æ·±åº¦
            max_pages: æœ€å¤§çˆ¬å–é¡µé¢æ•°

        Returns:
            æå–çš„äº§å“æ•°æ®åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹é€’å½’çˆ¬å–: {start_url}")
        logger.info(f"æœ€å¤§æ·±åº¦: {max_depth}, æœ€å¤§é¡µé¢æ•°: {max_pages}")

        await self._crawl_recursive_helper(start_url, 0, max_depth, max_pages)

        logger.info(f"çˆ¬å–å®Œæˆ: è®¿é—®{len(self.visited_urls)}é¡µ, æå–{len(self.products)}ä¸ªäº§å“")
        return self.products

    async def _crawl_recursive_helper(
        self,
        url: str,
        current_depth: int,
        max_depth: int,
        max_pages: int
    ):
        """é€’å½’çˆ¬å–è¾…åŠ©å‡½æ•°"""
        if current_depth > max_depth or len(self.visited_urls) >= max_pages:
            return

        # çˆ¬å–å½“å‰é¡µé¢
        page_result = await self.crawl_page(url, current_depth)
        if not page_result:
            return

        # å»¶è¿Ÿ
        await asyncio.sleep(1)

        # é€’å½’çˆ¬å–é“¾æ¥
        if current_depth < max_depth:
            result = page_result.get('result')
            if result and hasattr(result, 'links') and result.links:
                base_domain = urlparse(url).netloc
                internal_links = result.links.get('internal', [])

                # æå–URL
                urls = []
                for link in internal_links:
                    if isinstance(link, dict):
                        link_url = link.get('href', '')
                    else:
                        link_url = str(link)

                    if link_url:
                        full_url = urljoin(url, link_url)
                        normalized = self._normalize_url(full_url)

                        if normalized not in self.visited_urls and self._is_valid_url(normalized, base_domain):
                            urls.append(full_url)

                # é™åˆ¶é“¾æ¥æ•°
                for link_url in urls[:5]:
                    if len(self.visited_urls) >= max_pages:
                        break
                    await self._crawl_recursive_helper(link_url, current_depth + 1, max_depth, max_pages)

    def save_results(self):
        """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
        if not self.products:
            logger.warning("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return None

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = self.output_dir / f"products_{timestamp}.json"

        output = {
            "template": self.template,
            "crawl_info": {
                "pages_visited": len(self.visited_urls),
                "products_found": len(self.products),
                "crawled_at": datetime.now().isoformat()
            },
            "products": self.products
        }

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ’¾ å·²ä¿å­˜åˆ°: {filename}")
        return filename

    def print_summary(self):
        """æ‰“å°æ‘˜è¦"""
        print("\n" + "="*60)
        print("çˆ¬å–æ‘˜è¦")
        print("="*60)
        print(f"è®¿é—®é¡µé¢: {len(self.visited_urls)}")
        print(f"æå–äº§å“: {len(self.products)}")

        if self.products:
            print(f"\næå–çš„æ•°æ®:")
            for i, product in enumerate(self.products, 1):
                print(f"\nã€äº§å“ {i}ã€‘")
                print(f"æ¥æº: {product.get('_source_url', 'Unknown')}")
                for key, value in product.items():
                    if not key.startswith('_'):
                        print(f"  {key}: {value}")

        print("\n" + "="*60)


async def main():
    """ä¸»å‡½æ•° - ä»config.pyè¯»å–é…ç½®çˆ¬å–"""
    print("\nğŸ¤– æ™ºèƒ½äº§å“ä¿¡æ¯çˆ¬è™«")
    print("="*60)

    # åˆ›å»ºçˆ¬è™« - è‡ªåŠ¨ä»config.pyè¯»å–é…ç½®
    crawler = SmartCrawler(
        template_path=CRAWLER_CONFIG.get("template_path"),
        llm_config_key="deepseek"  # ä½¿ç”¨config.pyä¸­çš„deepseeké…ç½®
    )

    # ä»configè¯»å–é…ç½®
    url = CRAWLER_CONFIG.get("start_url", "https://api-docs.deepseek.com/zh-cn/quick_start/pricing")
    enable_recursive = CRAWLER_CONFIG.get("enable_recursive", False)
    max_depth = CRAWLER_CONFIG.get("max_depth", 2)
    max_pages = CRAWLER_CONFIG.get("max_pages", 20)

    print(f"\nç›®æ ‡URL: {url}")
    print(f"æ¨¡æ¿æ–‡ä»¶: {crawler.template_path}")
    print(f"LLM Provider: {crawler.provider}")
    print(f"é€’å½’çˆ¬å–: {'å¯ç”¨' if enable_recursive else 'ç¦ç”¨'}")
    if enable_recursive:
        print(f"æœ€å¤§æ·±åº¦: {max_depth}, æœ€å¤§é¡µé¢æ•°: {max_pages}")
    print()

    # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦é€’å½’çˆ¬å–
    if enable_recursive:
        await crawler.crawl_recursive(url, max_depth=max_depth, max_pages=max_pages)
    else:
        await crawler.crawl_page(url, depth=0)

    # ä¿å­˜ç»“æœ
    crawler.save_results()

    # æ‰“å°æ‘˜è¦
    crawler.print_summary()

    print("\nğŸ‰ å®Œæˆï¼\n")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"ğŸš¨ ç¨‹åºå¼‚å¸¸: {e}", exc_info=True)
