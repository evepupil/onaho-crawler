#!/usr/bin/env python3
"""
å®Œæ•´çš„çˆ¬è™«å·¥ä½œæµç³»ç»Ÿ
æä¾›ä»»åŠ¡ç®¡ç†ã€è°ƒåº¦ã€ç»“æœå­˜å‚¨ç­‰å®Œæ•´åŠŸèƒ½
"""

import asyncio
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
import logging
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.smart_crawler import SmartCrawler
from configs.config import LLM_CONFIG, CRAWLER_CONFIG

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class CrawlerTask:
    """çˆ¬è™«ä»»åŠ¡"""

    def __init__(
        self,
        task_id: str,
        name: str,
        start_url: str,
        template_path: str,
        config: Dict = None
    ):
        """
        åˆå§‹åŒ–çˆ¬è™«ä»»åŠ¡

        Args:
            task_id: ä»»åŠ¡ID
            name: ä»»åŠ¡åç§°
            start_url: èµ·å§‹URL
            template_path: æ¨¡æ¿æ–‡ä»¶è·¯å¾„
            config: ä»»åŠ¡é…ç½®ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨CRAWLER_CONFIGï¼‰
        """
        self.task_id = task_id
        self.name = name
        self.start_url = start_url
        self.template_path = template_path
        self.config = config or CRAWLER_CONFIG.copy()

        # ä»»åŠ¡çŠ¶æ€
        self.status = "pending"  # pending, running, completed, failed
        self.created_at = datetime.now().isoformat()
        self.started_at = None
        self.completed_at = None
        self.error = None

        # ç»“æœ
        self.result_file = None
        self.pages_visited = 0
        self.products_found = 0

    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "task_id": self.task_id,
            "name": self.name,
            "start_url": self.start_url,
            "template_path": self.template_path,
            "config": self.config,
            "status": self.status,
            "created_at": self.created_at,
            "started_at": self.started_at,
            "completed_at": self.completed_at,
            "error": self.error,
            "result_file": str(self.result_file) if self.result_file else None,
            "pages_visited": self.pages_visited,
            "products_found": self.products_found
        }

    @classmethod
    def from_dict(cls, data: Dict) -> 'CrawlerTask':
        """ä»å­—å…¸åˆ›å»ºä»»åŠ¡"""
        task = cls(
            task_id=data["task_id"],
            name=data["name"],
            start_url=data["start_url"],
            template_path=data["template_path"],
            config=data.get("config")
        )
        task.status = data.get("status", "pending")
        task.created_at = data.get("created_at")
        task.started_at = data.get("started_at")
        task.completed_at = data.get("completed_at")
        task.error = data.get("error")
        task.result_file = Path(data["result_file"]) if data.get("result_file") else None
        task.pages_visited = data.get("pages_visited", 0)
        task.products_found = data.get("products_found", 0)
        return task


class TaskQueue:
    """ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†"""

    def __init__(self, storage_path: str = None):
        """
        åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ—

        Args:
            storage_path: ä»»åŠ¡å­˜å‚¨æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: data/tasks.jsonï¼‰
        """
        if storage_path is None:
            # é»˜è®¤å­˜å‚¨åœ¨ data ç›®å½•
            project_root = Path(__file__).parent.parent
            storage_path = project_root / "data" / "tasks.json"

        self.storage_path = Path(storage_path)
        self.storage_path.parent.mkdir(parents=True, exist_ok=True)
        self.tasks: Dict[str, CrawlerTask] = {}
        self.load()

    def add_task(self, task: CrawlerTask):
        """æ·»åŠ ä»»åŠ¡"""
        self.tasks[task.task_id] = task
        self.save()
        logger.info(f"æ·»åŠ ä»»åŠ¡: {task.task_id} - {task.name}")

    def get_task(self, task_id: str) -> Optional[CrawlerTask]:
        """è·å–ä»»åŠ¡"""
        return self.tasks.get(task_id)

    def get_pending_tasks(self) -> List[CrawlerTask]:
        """è·å–å¾…æ‰§è¡Œä»»åŠ¡"""
        return [t for t in self.tasks.values() if t.status == "pending"]

    def update_task(self, task: CrawlerTask):
        """æ›´æ–°ä»»åŠ¡"""
        self.tasks[task.task_id] = task
        self.save()

    def save(self):
        """ä¿å­˜ä»»åŠ¡é˜Ÿåˆ—åˆ°æ–‡ä»¶"""
        data = {
            task_id: task.to_dict()
            for task_id, task in self.tasks.items()
        }
        with open(self.storage_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def load(self):
        """ä»æ–‡ä»¶åŠ è½½ä»»åŠ¡é˜Ÿåˆ—"""
        if not self.storage_path.exists():
            return

        with open(self.storage_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        self.tasks = {
            task_id: CrawlerTask.from_dict(task_data)
            for task_id, task_data in data.items()
        }
        logger.info(f"åŠ è½½äº† {len(self.tasks)} ä¸ªä»»åŠ¡")

    def get_summary(self) -> Dict:
        """è·å–ä»»åŠ¡é˜Ÿåˆ—æ‘˜è¦"""
        return {
            "total": len(self.tasks),
            "pending": len([t for t in self.tasks.values() if t.status == "pending"]),
            "running": len([t for t in self.tasks.values() if t.status == "running"]),
            "completed": len([t for t in self.tasks.values() if t.status == "completed"]),
            "failed": len([t for t in self.tasks.values() if t.status == "failed"])
        }


class CrawlerWorkflow:
    """çˆ¬è™«å·¥ä½œæµç³»ç»Ÿ"""

    def __init__(
        self,
        task_queue: TaskQueue = None,
        llm_config_key: str = "deepseek",
        max_concurrent: int = 1
    ):
        """
        åˆå§‹åŒ–å·¥ä½œæµç³»ç»Ÿ

        Args:
            task_queue: ä»»åŠ¡é˜Ÿåˆ—ï¼ˆå¯é€‰ï¼‰
            llm_config_key: LLMé…ç½®é”®
            max_concurrent: æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°
        """
        self.task_queue = task_queue or TaskQueue()
        self.llm_config_key = llm_config_key
        self.max_concurrent = max_concurrent
        self.running_tasks = set()

    async def execute_task(self, task: CrawlerTask) -> bool:
        """
        æ‰§è¡Œå•ä¸ªä»»åŠ¡

        Args:
            task: çˆ¬è™«ä»»åŠ¡

        Returns:
            æ˜¯å¦æ‰§è¡ŒæˆåŠŸ
        """
        try:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task.status = "running"
            task.started_at = datetime.now().isoformat()
            self.task_queue.update_task(task)
            self.running_tasks.add(task.task_id)

            logger.info(f"å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task.task_id} - {task.name}")

            # åˆ›å»ºçˆ¬è™«å®ä¾‹
            crawler = SmartCrawler(
                template_path=task.template_path,
                output_dir=task.config.get("output_dir", "output"),
                llm_config_key=self.llm_config_key
            )

            # æ‰§è¡Œçˆ¬å–
            enable_recursive = task.config.get("enable_recursive", False)

            if enable_recursive:
                max_depth = task.config.get("max_depth", 2)
                max_pages = task.config.get("max_pages", 20)
                await crawler.crawl_recursive(
                    task.start_url,
                    max_depth=max_depth,
                    max_pages=max_pages
                )
            else:
                await crawler.crawl_page(task.start_url, depth=0)

            # ä¿å­˜ç»“æœ
            result_file = crawler.save_results()

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task.status = "completed"
            task.completed_at = datetime.now().isoformat()
            task.result_file = result_file
            task.pages_visited = len(crawler.visited_urls)
            task.products_found = len(crawler.products)

            logger.info(f"ä»»åŠ¡å®Œæˆ: {task.task_id} - è®¿é—®{task.pages_visited}é¡µï¼Œæå–{task.products_found}ä¸ªäº§å“")

            return True

        except Exception as e:
            logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {task.task_id} - {e}", exc_info=True)
            task.status = "failed"
            task.completed_at = datetime.now().isoformat()
            task.error = str(e)
            return False

        finally:
            self.task_queue.update_task(task)
            self.running_tasks.discard(task.task_id)

    async def run_pending_tasks(self):
        """è¿è¡Œæ‰€æœ‰å¾…æ‰§è¡Œä»»åŠ¡"""
        pending_tasks = self.task_queue.get_pending_tasks()

        if not pending_tasks:
            logger.info("æ²¡æœ‰å¾…æ‰§è¡Œçš„ä»»åŠ¡")
            return

        logger.info(f"å¼€å§‹æ‰§è¡Œ {len(pending_tasks)} ä¸ªå¾…æ‰§è¡Œä»»åŠ¡")

        # æ ¹æ®max_concurrentæ§åˆ¶å¹¶å‘
        if self.max_concurrent == 1:
            # ä¸²è¡Œæ‰§è¡Œ
            for task in pending_tasks:
                await self.execute_task(task)
        else:
            # å¹¶å‘æ‰§è¡Œ
            semaphore = asyncio.Semaphore(self.max_concurrent)

            async def execute_with_semaphore(task):
                async with semaphore:
                    await self.execute_task(task)

            await asyncio.gather(*[
                execute_with_semaphore(task)
                for task in pending_tasks
            ])

        logger.info("æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæˆ")

    def create_task_from_config(
        self,
        task_id: str,
        name: str,
        start_url: str = None,
        template_path: str = None,
        **kwargs
    ) -> CrawlerTask:
        """
        ä»é…ç½®åˆ›å»ºä»»åŠ¡

        Args:
            task_id: ä»»åŠ¡ID
            name: ä»»åŠ¡åç§°
            start_url: èµ·å§‹URLï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»CRAWLER_CONFIGè¯»å–ï¼‰
            template_path: æ¨¡æ¿è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»CRAWLER_CONFIGè¯»å–ï¼‰
            **kwargs: å…¶ä»–é…ç½®è¦†ç›–

        Returns:
            åˆ›å»ºçš„ä»»åŠ¡
        """
        config = CRAWLER_CONFIG.copy()
        config.update(kwargs)

        task = CrawlerTask(
            task_id=task_id,
            name=name,
            start_url=start_url or config.get("start_url"),
            template_path=template_path or config.get("template_path"),
            config=config
        )

        self.task_queue.add_task(task)
        return task

    def print_summary(self):
        """æ‰“å°å·¥ä½œæµæ‘˜è¦"""
        summary = self.task_queue.get_summary()

        print("\n" + "="*60)
        print("çˆ¬è™«å·¥ä½œæµæ‘˜è¦")
        print("="*60)
        print(f"æ€»ä»»åŠ¡æ•°: {summary['total']}")
        print(f"  å¾…æ‰§è¡Œ: {summary['pending']}")
        print(f"  æ‰§è¡Œä¸­: {summary['running']}")
        print(f"  å·²å®Œæˆ: {summary['completed']}")
        print(f"  å¤±è´¥: {summary['failed']}")

        print("\nä»»åŠ¡åˆ—è¡¨:")
        for task in self.task_queue.tasks.values():
            status_icon = {
                "pending": "â³",
                "running": "ğŸ”„",
                "completed": "âœ…",
                "failed": "âŒ"
            }.get(task.status, "â“")

            print(f"\n{status_icon} [{task.task_id}] {task.name}")
            print(f"   URL: {task.start_url}")
            print(f"   çŠ¶æ€: {task.status}")
            if task.status == "completed":
                print(f"   ç»“æœ: {task.pages_visited}é¡µ, {task.products_found}ä¸ªäº§å“")
                print(f"   æ–‡ä»¶: {task.result_file}")
            elif task.status == "failed":
                print(f"   é”™è¯¯: {task.error}")

        print("\n" + "="*60)


async def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå·¥ä½œæµä½¿ç”¨"""
    print("\nğŸ¤– çˆ¬è™«å·¥ä½œæµç³»ç»Ÿ")
    print("="*60)

    # åˆ›å»ºå·¥ä½œæµ
    workflow = CrawlerWorkflow(
        llm_config_key="deepseek",
        max_concurrent=1  # ä¸²è¡Œæ‰§è¡Œ
    )

    # æ–¹æ¡ˆ1: åˆ›å»ºå¤šä¸ªä»»åŠ¡
    print("\nåˆ›å»ºç¤ºä¾‹ä»»åŠ¡...")

    # ä»»åŠ¡1: DeepSeekå®šä»·ï¼ˆé€’å½’çˆ¬å–ï¼‰
    workflow.create_task_from_config(
        task_id="task_001",
        name="DeepSeekå®šä»·ä¿¡æ¯",
        start_url="https://api-docs.deepseek.com/zh-cn/",
        template_path="template_deepseek_pricing.json",
        enable_recursive=True,
        max_depth=2,
        max_pages=10
    )

    # ä»»åŠ¡2: DeepSeekå®šä»·ï¼ˆå•é¡µçˆ¬å–ï¼‰
    workflow.create_task_from_config(
        task_id="task_002",
        name="DeepSeekå®šä»·ä¿¡æ¯ï¼ˆå•é¡µï¼‰",
        start_url="https://api-docs.deepseek.com/zh-cn/quick_start/pricing",
        template_path="template_deepseek_pricing.json",
        enable_recursive=False
    )

    # æ‰“å°æ‘˜è¦
    workflow.print_summary()

    # æ‰§è¡Œæ‰€æœ‰å¾…æ‰§è¡Œä»»åŠ¡
    print("\nå¼€å§‹æ‰§è¡Œä»»åŠ¡...")
    await workflow.run_pending_tasks()

    # æ‰“å°æœ€ç»ˆæ‘˜è¦
    workflow.print_summary()

    print("\nğŸ‰ å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼\n")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"ğŸš¨ ç¨‹åºå¼‚å¸¸: {e}", exc_info=True)
