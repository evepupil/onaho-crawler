#!/usr/bin/env python3
"""
äº§å“ä¿¡æ¯çˆ¬è™«
ä½¿ç”¨LLMæ ¹æ®æ¨¡æ¿æå–ç»“æ„åŒ–æ•°æ®
"""

import asyncio
import json
import os
from datetime import datetime
from pathlib import Path
from urllib.parse import urljoin, urlparse

try:
    from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
    from crawl4ai.extraction_strategy import LLMExtractionStrategy
except ImportError:
    print("è¯·å…ˆå®‰è£…crawl4ai: pip install crawl4ai")
    exit(1)

# å¯¼å…¥é…ç½®
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))
from configs.config import LLM_CONFIG, CRAWLER_CONFIG

import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('product_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class ProductCrawler:
    """äº§å“ä¿¡æ¯çˆ¬è™« - ä½¿ç”¨LLMæå–ç»“æ„åŒ–æ•°æ®"""

    def __init__(
        self,
        template_path: str = None,
        output_dir: str = None,
        api_token: str = None,
        provider: str = None,
        llm_config_key: str = "deepseek"
    ):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            template_path: JSONæ¨¡æ¿æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»configè¯»å–ï¼‰
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»configè¯»å–ï¼‰
            api_token: LLM API tokenï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»configè¯»å–ï¼‰
            provider: LLM providerï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»configè¯»å–ï¼‰
            llm_config_key: config.pyä¸­LLM_CONFIGçš„é”®åï¼ˆé»˜è®¤"deepseek"ï¼‰
        """
        # ä»configè¯»å–LLMé…ç½®
        llm_cfg = LLM_CONFIG.get(llm_config_key, {})

        self.template_path = template_path or CRAWLER_CONFIG.get("template_path", "template_deepseek_pricing.json")
        self.output_dir = Path(output_dir or CRAWLER_CONFIG.get("output_dir", "output"))
        self.output_dir.mkdir(exist_ok=True)

        # åŠ è½½æ¨¡æ¿
        with open(self.template_path, 'r', encoding='utf-8') as f:
            self.template = json.load(f)

        # LLMé…ç½® - ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å‚æ•°ï¼Œå¦åˆ™ä»configè¯»å–
        self.api_token = api_token or llm_cfg.get("api_token")
        self.provider = provider or llm_cfg.get("provider", "openai/gpt-4o-mini")

        logger.info(f"ä»config.pyè¯»å–é…ç½®: provider={self.provider}, api_token={'å·²è®¾ç½®' if self.api_token else 'æœªè®¾ç½®'}")

        # çˆ¬å–ç»Ÿè®¡
        self.visited_urls = set()
        self.products_data = []

    def _create_extraction_instruction(self) -> str:
        """æ ¹æ®æ¨¡æ¿åˆ›å»ºLLMæå–æŒ‡ä»¤"""
        template_str = json.dumps(self.template, ensure_ascii=False, indent=2)

        instruction = f"""
ä»ç½‘é¡µå†…å®¹ä¸­æå–äº§å“ä¿¡æ¯ï¼Œä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ã€‚

æ¨¡æ¿å®šä¹‰ï¼ˆé”®æ˜¯å­—æ®µåï¼Œå€¼æ˜¯å­—æ®µè¯´æ˜ï¼‰ï¼š
{template_str}

**é‡è¦è¦æ±‚**ï¼š
1. ç›´æ¥è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼Œä¸è¦è¿”å›æ•°ç»„æˆ–å…¶ä»–æ ¼å¼
2. JSONå¯¹è±¡çš„é”®å¿…é¡»å®Œå…¨åŒ¹é…æ¨¡æ¿ä¸­çš„é”®å
3. æ¯ä¸ªé”®çš„å€¼æ˜¯ä»ç½‘é¡µä¸­æå–çš„å®é™…æ•°æ®
4. å¦‚æœæŸä¸ªå­—æ®µåœ¨ç½‘é¡µä¸­æ‰¾ä¸åˆ°ï¼Œè®¾ç½®ä¸º null
5. ä¸è¦æ·»åŠ  index, tags, content, error ç­‰é¢å¤–å­—æ®µ
6. ä¸è¦ä½¿ç”¨ blocks æ ¼å¼
7. åªè¿”å›çº¯JSONå¯¹è±¡ï¼Œä¸è¦ä»»ä½•å…¶ä»–å†…å®¹

å¿…é¡»ä¸¥æ ¼éµå®ˆçš„è¿”å›æ ¼å¼ç¤ºä¾‹ï¼š
{{
{chr(10).join([f'  "{k}": "å®é™…æå–çš„å€¼"' for k in self.template.keys()])}
}}

å†æ¬¡å¼ºè°ƒï¼šç›´æ¥è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«æ¨¡æ¿ä¸­å®šä¹‰çš„æ‰€æœ‰å­—æ®µï¼Œä¸è¦ä»»ä½•åŒ…è£…æˆ–é¢å¤–ç»“æ„ã€‚
"""
        return instruction

    def _normalize_url(self, url: str) -> str:
        """æ ‡å‡†åŒ–URL"""
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}{parsed.path}"

    def _is_valid_url(self, url: str, base_domain: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ"""
        try:
            parsed = urlparse(url)
            if parsed.scheme not in ['http', 'https']:
                return False
            return parsed.netloc == base_domain
        except:
            return False

    async def crawl_single_page(self, url: str, depth: int = 0):
        """çˆ¬å–å•ä¸ªé¡µé¢å¹¶ä½¿ç”¨LLMæå–æ•°æ®"""
        normalized_url = self._normalize_url(url)

        if normalized_url in self.visited_urls:
            return None

        self.visited_urls.add(normalized_url)
        logger.info(f"[æ·±åº¦{depth}] æ­£åœ¨çˆ¬å–: {normalized_url}")

        # åˆ›å»ºLLMæå–ç­–ç•¥ - ä½¿ç”¨æ–°çš„API
        llm_config = LLMConfig(
            provider=self.provider,
            api_token=self.api_token
        )

        extraction_strategy = LLMExtractionStrategy(
            llm_config=llm_config,
            instruction=self._create_extraction_instruction(),
            verbose=True
        )

        # æµè§ˆå™¨é…ç½®
        browser_config = BrowserConfig(
            headless=True,
            verbose=False
        )

        # çˆ¬å–é…ç½®
        run_config = CrawlerRunConfig(
            extraction_strategy=extraction_strategy,
            cache_mode=CacheMode.BYPASS
        )

        async with AsyncWebCrawler(config=browser_config) as crawler:
            try:
                result = await crawler.arun(
                    url=normalized_url,
                    config=run_config
                )

                if result.success:
                    # è§£æLLMæå–çš„JSON
                    if result.extracted_content:
                        try:
                            logger.info(f"LLMè¿”å›å†…å®¹: {result.extracted_content[:500]}")
                            extracted_data = json.loads(result.extracted_content)

                            # æ£€æŸ¥è¿”å›æ ¼å¼å¹¶æå–æ­£ç¡®çš„æ•°æ®
                            if isinstance(extracted_data, list):
                                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯blocksæ ¼å¼
                                if len(extracted_data) > 0:
                                    first_item = extracted_data[0] if isinstance(extracted_data[0], dict) else {}

                                    # æ£€æŸ¥æ˜¯å¦æ˜¯blocksæ ¼å¼ï¼ˆæœ‰index/tags/contentå­—æ®µï¼‰
                                    if 'index' in first_item and 'content' in first_item:
                                        logger.warning("âš ï¸ LLMè¿”å›äº†blocksæ ¼å¼ï¼Œè€Œä¸æ˜¯æ¨¡æ¿æ ¼å¼ï¼Œè·³è¿‡æ­¤ç»“æœ")
                                        return None

                                    # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯ä¿¡æ¯
                                    if first_item.get('error') is True or 'error' in first_item.get('tags', []):
                                        error_msg = first_item.get('content', 'æœªçŸ¥é”™è¯¯')
                                        logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {error_msg}")
                                        return None

                                    extracted_data = first_item
                                else:
                                    extracted_data = {}
                            elif isinstance(extracted_data, dict):
                                # æ£€æŸ¥æ˜¯å¦æ˜¯blocksæ ¼å¼
                                if 'index' in extracted_data and 'content' in extracted_data:
                                    logger.warning("âš ï¸ LLMè¿”å›äº†blocksæ ¼å¼ï¼Œè€Œä¸æ˜¯æ¨¡æ¿æ ¼å¼ï¼Œè·³è¿‡æ­¤ç»“æœ")
                                    return None

                                # æ£€æŸ¥å­—å…¸æ˜¯å¦åŒ…å«é”™è¯¯
                                if extracted_data.get('error') is True:
                                    error_msg = extracted_data.get('content', extracted_data.get('message', 'æœªçŸ¥é”™è¯¯'))
                                    logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {error_msg}")
                                    return None
                            else:
                                logger.error(f"âŒ æ„å¤–çš„è¿”å›æ ¼å¼: {type(extracted_data)}")
                                return None

                            # éªŒè¯è¿”å›çš„æ•°æ®æ˜¯å¦åŒ…å«æ¨¡æ¿å­—æ®µ
                            template_keys = set(self.template.keys())
                            data_keys = set(k for k in extracted_data.keys() if not k.startswith('_'))

                            # å¦‚æœè¿”å›çš„å­—æ®µå’Œæ¨¡æ¿å­—æ®µå®Œå…¨ä¸åŒ¹é…ï¼Œè¯´æ˜æ ¼å¼ä¸å¯¹
                            if not template_keys.intersection(data_keys):
                                logger.warning(f"âš ï¸ è¿”å›çš„å­—æ®µä¸æ¨¡æ¿ä¸åŒ¹é…ã€‚æ¨¡æ¿å­—æ®µ: {template_keys}, è¿”å›å­—æ®µ: {data_keys}")
                                return None

                            # æ·»åŠ æºURL
                            extracted_data['_source_url'] = normalized_url
                            extracted_data['_crawled_at'] = datetime.now().isoformat()

                            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
                            has_data = any(
                                value is not None and value != ""
                                for key, value in extracted_data.items()
                                if not key.startswith('_')
                            )

                            if has_data:
                                self.products_data.append(extracted_data)
                                logger.info(f"âœ… æˆåŠŸæå–æ•°æ®: {normalized_url}")
                            else:
                                logger.warning(f"æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®: {normalized_url}")

                            # æ— è®ºæ˜¯å¦æ‰¾åˆ°æœ‰æ•ˆæ•°æ®ï¼Œéƒ½è¿”å›resultä»¥ä¾¿ç»§ç»­é€’å½’
                            return {
                                'data': extracted_data if has_data else None,
                                'result': result,
                                'url': normalized_url
                            }

                        except json.JSONDecodeError as e:
                            logger.error(f"JSONè§£æå¤±è´¥: {normalized_url} - {e}")
                            logger.error(f"è¿”å›å†…å®¹: {result.extracted_content[:500]}")
                            return None
                    else:
                        logger.warning(f"LLMæœªè¿”å›å†…å®¹: {normalized_url}")
                        return None
                else:
                    logger.error(f"âŒ çˆ¬å–å¤±è´¥: {normalized_url} - {result.error_message}")
                    return None

            except Exception as e:
                logger.error(f"ğŸš¨ çˆ¬å–å¼‚å¸¸: {normalized_url} - {e}")
                return None

    async def recursive_crawl(self, start_url: str, max_depth: int = 2, max_pages: int = 10):
        """é€’å½’çˆ¬å–"""
        logger.info(f"å¼€å§‹é€’å½’çˆ¬å–ï¼Œèµ·å§‹URL: {start_url}, æœ€å¤§æ·±åº¦: {max_depth}")

        await self._crawl_recursive_helper(start_url, 0, max_depth, max_pages)

        logger.info(f"é€’å½’çˆ¬å–å®Œæˆï¼Œå…±è®¿é—® {len(self.visited_urls)} ä¸ªé¡µé¢ï¼Œæå– {len(self.products_data)} æ¡æ•°æ®")
        return self.products_data

    async def _crawl_recursive_helper(self, url: str, current_depth: int, max_depth: int, max_pages: int):
        """é€’å½’çˆ¬å–è¾…åŠ©å‡½æ•°"""
        if current_depth > max_depth:
            return

        if len(self.visited_urls) >= max_pages:
            return

        # çˆ¬å–å½“å‰é¡µé¢
        page_result = await self.crawl_single_page(url, current_depth)
        if not page_result:
            return

        # å»¶è¿Ÿ
        await asyncio.sleep(1)

        # é€’å½’çˆ¬å–é“¾æ¥
        if current_depth < max_depth and page_result.get('result'):
            result = page_result['result']
            if hasattr(result, 'links') and result.links:
                base_domain = urlparse(url).netloc
                all_links = []

                # æå–å†…éƒ¨é“¾æ¥
                for link in result.links.get('internal', []):
                    if isinstance(link, dict):
                        link_url = link.get('href', '')
                    else:
                        link_url = str(link)

                    if link_url:
                        full_url = urljoin(url, link_url)
                        normalized_url = self._normalize_url(full_url)

                        if normalized_url not in self.visited_urls and self._is_valid_url(normalized_url, base_domain):
                            all_links.append(full_url)

                # é™åˆ¶é“¾æ¥æ•°é‡
                for link in all_links[:10]:
                    if len(self.visited_urls) >= max_pages:
                        break

                    await self._crawl_recursive_helper(link, current_depth + 1, max_depth, max_pages)

    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        if not self.products_data:
            logger.warning("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = self.output_dir / f"products_{timestamp}.json"

        output_data = {
            "template": self.template,
            "crawl_info": {
                "total_pages_visited": len(self.visited_urls),
                "total_products_found": len(self.products_data),
                "crawled_at": datetime.now().isoformat()
            },
            "products": self.products_data
        }

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
        return filename

    def print_summary(self):
        """æ‰“å°æ‘˜è¦"""
        print("\n" + "="*60)
        print("çˆ¬å–æ‘˜è¦")
        print("="*60)
        print(f"ğŸ“„ è®¿é—®é¡µé¢æ•°: {len(self.visited_urls)}")
        print(f"ğŸ“Š æå–äº§å“æ•°: {len(self.products_data)}")

        if self.products_data:
            print(f"\næå–çš„äº§å“:")
            for i, product in enumerate(self.products_data, 1):
                print(f"\n{i}. æ¥æº: {product.get('_source_url', 'Unknown')}")
                for key, value in product.items():
                    if not key.startswith('_'):
                        print(f"   {key}: {value}")

        print(f"\nğŸ’¾ æ•°æ®ä¿å­˜ä½ç½®: {self.output_dir}")
        print("="*60)


async def main():
    """ä¸»å‡½æ•° - ä»config.pyè¯»å–é…ç½®çˆ¬å–"""
    print("ğŸ¤– å¼€å§‹çˆ¬å–äº§å“ä¿¡æ¯...")
    print("="*60)

    # åˆ›å»ºçˆ¬è™« - è‡ªåŠ¨ä»config.pyè¯»å–æ‰€æœ‰é…ç½®
    crawler = ProductCrawler(
        llm_config_key="deepseek"  # ä½¿ç”¨config.pyä¸­çš„deepseeké…ç½®
    )

    # ä»configè¯»å–é…ç½®
    start_url = CRAWLER_CONFIG.get("start_url", "https://api-docs.deepseek.com/zh-cn/")
    enable_recursive = CRAWLER_CONFIG.get("enable_recursive", False)
    max_depth = CRAWLER_CONFIG.get("max_depth", 2)
    max_pages = CRAWLER_CONFIG.get("max_pages", 20)

    print(f"ç›®æ ‡URL: {start_url}")
    print(f"æ¨¡æ¿æ–‡ä»¶: {crawler.template_path}")
    print(f"LLM Provider: {crawler.provider}")
    print(f"é€’å½’çˆ¬å–: {'å¯ç”¨' if enable_recursive else 'ç¦ç”¨'}")
    if enable_recursive:
        print(f"æœ€å¤§æ·±åº¦: {max_depth}, æœ€å¤§é¡µé¢æ•°: {max_pages}")
    print()

    # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦é€’å½’çˆ¬å–
    if enable_recursive:
        await crawler.recursive_crawl(start_url, max_depth=max_depth, max_pages=max_pages)
    else:
        await crawler.crawl_single_page(start_url, depth=0)

    # ä¿å­˜ç»“æœ
    crawler.save_results()

    # æ‰“å°æ‘˜è¦
    crawler.print_summary()

    print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ çˆ¬å–è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"ğŸš¨ ç¨‹åºå¼‚å¸¸: {e}")
        print(f"ğŸš¨ ç¨‹åºå¼‚å¸¸: {e}")
