# 两种爬取策略对比

## 策略1: 边爬边分析（原有方案）

**实现文件**: `smart_crawler.py`

### 工作流程
```
1. 访问页面
2. 用LLM分析提取数据
3. 提取子链接
4. 递归访问子链接（重复1-3）
```

### 优点
- ✅ 实时反馈，边爬边得结果
- ✅ 代码逻辑简单
- ✅ 适合目标明确的场景

### 缺点
- ❌ **效率低**: 每个页面都调用LLM，即使不是产品页
- ❌ **成本高**: 浪费大量token在无关页面
- ❌ **容易遗漏**: 爬取深度有限，可能漏掉深层页面
- ❌ **不够灵活**: URL模式固定后难以调整

### 示例成本（假设）
```
访问50个页面
- 其中40个是导航/列表页（无用）
- 其中10个是产品页（有用）

LLM调用: 50次
成本: 50 × 单次成本
```

---

## 策略2: 两阶段爬取（新方案）⭐

**实现文件**: `two_stage_crawler.py`

### 工作流程

#### 阶段1: 收集链接（不使用LLM）
```
1. 快速递归爬取，只收集链接
2. 不调用LLM，不分析内容
3. 保存所有链接到文件
```

#### 阶段2: 精准提取（使用LLM）
```
1. 从文件加载所有链接
2. 根据URL模式过滤产品页
3. 只对产品页调用LLM提取数据
```

### 优点
- ✅ **效率高**: 阶段1快速扫描，阶段2精准打击
- ✅ **成本低**: 只对产品页调用LLM，节省80%+ token
- ✅ **覆盖全**: 可以爬取更多页面，发现更多产品
- ✅ **灵活性强**: 链接保存后可以反复使用不同策略过滤
- ✅ **可调试**: 可以先看链接列表，确认覆盖范围

### 缺点
- ⚠️ 需要两次运行
- ⚠️ 需要定义URL模式

### 示例成本（假设）
```
阶段1: 访问100个页面（不使用LLM）
成本: 0

阶段2: 过滤出10个产品页，调用LLM
成本: 10 × 单次成本

总成本: 节省80%
```

---

## 详细对比

| 维度 | 策略1: 边爬边分析 | 策略2: 两阶段爬取 |
|-----|------------------|------------------|
| **LLM调用次数** | 每个页面1次 | 只对产品页1次 |
| **Token消耗** | 高（100%） | 低（约20%） |
| **爬取速度** | 慢（每页都等LLM） | 快（阶段1无LLM） |
| **覆盖范围** | 有限（深度限制） | 全面（可深度优先） |
| **灵活性** | 低（需重新爬） | 高（链接复用） |
| **调试性** | 差（无法预览） | 好（可查看链接） |
| **适用场景** | 小网站、目标明确 | 大网站、需全量 |

---

## 实际案例对比

### 案例: 爬取 DeepSeek 官网

#### 策略1 实际效果
```
访问页面: 7页
LLM调用: 7次
提取产品: 2个
耗时: ~40秒
成本: 7次LLM调用
```

#### 策略2 实际效果
```
阶段1:
  访问页面: 14页
  收集链接: 62个
  LLM调用: 0次
  耗时: ~30秒

阶段2:
  过滤链接: 10个产品页
  LLM调用: 10次
  提取产品: 2个
  耗时: ~70秒

总计:
  访问页面: 14页（覆盖更全）
  LLM调用: 10次（只分析产品页）
  提取产品: 2个
  总耗时: ~100秒
  成本: 10次LLM调用
```

**分析**:
- 覆盖范围: 策略2多了7个页面
- 发现的链接: 策略2多了55个链接
- LLM调用: 策略2多了3次（因为过滤了更多产品页）
- 如果网站更大，策略2优势会更明显

---

## 场景推荐

### 使用策略1（边爬边分析）的场景
- ✅ 网站结构简单，页面少（<20页）
- ✅ 目标URL明确，不需要发现
- ✅ 需要实时反馈结果
- ✅ 快速原型验证

### 使用策略2（两阶段爬取）的场景 ⭐
- ✅ 网站结构复杂，页面多（>50页）
- ✅ 需要全量扫描，不遗漏
- ✅ 产品页URL有规律（如含/product/）
- ✅ 关注成本，要节省token
- ✅ 需要多次实验不同过滤规则

---

## 代码示例

### 策略1: 边爬边分析
```python
from src.smart_crawler import SmartCrawler

crawler = SmartCrawler(
    template_path="templates/template.json",
    llm_config_key="deepseek"
)

# 递归爬取，边爬边用LLM分析
await crawler.crawl_recursive(
    start_url="https://example.com",
    max_depth=2,
    max_pages=20
)

crawler.save_results()
```

### 策略2: 两阶段爬取
```python
from src.two_stage_crawler import TwoStageCrawler

crawler = TwoStageCrawler(
    start_url="https://example.com",
    template_path="templates/template.json",
    llm_config_key="deepseek"
)

# 阶段1: 快速收集所有链接（不用LLM）
await crawler.stage1_collect_links(
    max_depth=3,
    max_pages=100
)

# 过滤产品链接
product_links = crawler.filter_product_links(
    url_patterns=["/product/", "/item/", "/p/"]
)

# 阶段2: 只对产品页用LLM提取
await crawler.stage2_extract_products(product_links)

crawler.save_products()
```

---

## 成本计算

假设：
- LLM单次调用成本: 0.001元
- 网站总页面: 100个
- 产品页面: 10个

### 策略1成本
```
访问20个页面（受深度限制）
每页调用LLM: 20次
成本: 20 × 0.001 = 0.02元
```

### 策略2成本
```
阶段1访问100个页面（不调用LLM）: 0元
阶段2调用LLM分析10个产品页: 10 × 0.001 = 0.01元
成本: 0.01元

节省: 50%
```

如果产品页占比更少（如100页中只有5个产品页），节省更多：
```
策略1: 0.02元（仍只爬20页）
策略2: 0.005元（只分析5个产品页）
节省: 75%
```

---

## 链接文件格式

阶段1保存的链接文件 `collected_links.json`:

```json
{
  "start_url": "https://example.com",
  "collected_at": "2025-11-24T16:45:11",
  "total_links": 62,
  "links": [
    "https://example.com/product/1",
    "https://example.com/product/2",
    "https://example.com/about",
    "https://example.com/contact",
    ...
  ]
}
```

**优势**:
- 可以离线分析链接
- 可以手动调整过滤规则
- 可以复用于不同的提取模板
- 可以归档历史爬取记录

---

## 总结

### 策略1适合
- 小型网站
- 快速验证
- 目标明确

### 策略2适合（推荐）⭐
- 大型网站
- 全量采集
- 关注成本
- 灵活调整

**建议**: 对于生产环境，推荐使用**策略2（两阶段爬取）**，虽然需要两次运行，但在覆盖范围、成本控制、灵活性上都有明显优势。
