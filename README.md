# æ™ºèƒ½çˆ¬è™« - ä¸¤é˜¶æ®µçˆ¬å–ç³»ç»Ÿ

åŸºäº crawl4ai å’Œ LLM çš„æ™ºèƒ½çˆ¬è™«ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µçˆ¬å–ç­–ç•¥ï¼ŒèŠ‚çœtokenæˆæœ¬ï¼Œæ”¯æŒæ‰¹æ¬¡çˆ¬å–å’Œæ–­ç‚¹ç»­çˆ¬ã€‚

## æ ¸å¿ƒç‰¹æ€§

âœ… **ä¸¤é˜¶æ®µçˆ¬å–ç­–ç•¥**ï¼ˆçœ80%+ tokenï¼‰
- é˜¶æ®µ1: å¿«é€Ÿé€’å½’çˆ¬å–æ‰€æœ‰é“¾æ¥ï¼ˆä¸ä½¿ç”¨LLMï¼Œä¸èŠ±é’±ï¼‰
- é˜¶æ®µ2: æ ¹æ®URLæ¨¡å¼è¿‡æ»¤äº§å“é¡µï¼Œè¯¦ç»†çˆ¬å–å¹¶LLMåˆ†æ

âœ… **æ‰¹æ¬¡çˆ¬å–å’Œæ–­ç‚¹ç»­çˆ¬**
- é“¾æ¥çŠ¶æ€è·Ÿè¸ªï¼ˆå·²çˆ¬å–/æœªçˆ¬å–ï¼‰
- æ”¯æŒä¸­æ–­åç»§ç»­
- æ‰¹æ¬¡å¤„ç†å¤§å‹ç½‘ç«™
- è¿›åº¦è‡ªåŠ¨ä¿å­˜

âœ… **æ™ºèƒ½æ•°æ®æå–**
- åŸºäºæ¨¡æ¿çš„ç»“æ„åŒ–è¾“å‡º
- LLM é©±åŠ¨çš„æ•°æ®æå–
- è‡ªåŠ¨å»é‡å’ŒéªŒè¯

## ç›®å½•ç»“æ„

```
crawl4ai_data_crawl/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ two_stage_crawler.py    # ä¸¤é˜¶æ®µçˆ¬è™«æ ¸å¿ƒ â­
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ config.py                # ç³»ç»Ÿé…ç½®ï¼ˆLLMã€çˆ¬è™«å‚æ•°ï¼‰âš™ï¸
â”‚   â””â”€â”€ two_stage_tasks.json     # ä»»åŠ¡é…ç½®ç¤ºä¾‹
â”‚
â”œâ”€â”€ templates/                   # æ•°æ®æå–æ¨¡æ¿ ğŸ“‹
â”‚   â””â”€â”€ template_*.json
â”‚
â”œâ”€â”€ docs/                        # æ–‡æ¡£ ğŸ“š
â”‚   â”œâ”€â”€ BATCH_CRAWLING.md        # æ‰¹æ¬¡çˆ¬å–å’Œæ–­ç‚¹ç»­çˆ¬è¯¦ç»†è¯´æ˜
â”‚   â”œâ”€â”€ STRATEGY_COMPARISON.md   # ç­–ç•¥å¯¹æ¯”
â”‚   â””â”€â”€ DIRECTORY.md             # ç›®å½•ç»“æ„è¯´æ˜
â”‚
â”œâ”€â”€ output/                      # çˆ¬å–ç»“æœï¼ˆæŒ‰ä»»åŠ¡åç§°ç»„ç»‡ï¼‰ğŸ“Š
â”‚   â””â”€â”€ task_name/
â”‚       â”œâ”€â”€ .stage1_completed    # é˜¶æ®µ1å®Œæˆæ ‡å¿—
â”‚       â”œâ”€â”€ collected_links.json # æ”¶é›†çš„æ‰€æœ‰é“¾æ¥ï¼ˆå¸¦çŠ¶æ€ï¼‰
â”‚       â””â”€â”€ products.json        # æå–çš„äº§å“æ•°æ®
â”‚
â””â”€â”€ requirements.txt
```

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–
```bash
pip install -r requirements.txt
```

### 2. é…ç½® API Key
ç¼–è¾‘ `configs/config.py`ï¼Œè®¾ç½®ä½ çš„ LLM API Key:
```python
LLM_CONFIG = {
    "deepseek": {
        "provider": "deepseek/deepseek-chat",
        "api_token": "your-api-key-here",  # ä¿®æ”¹è¿™é‡Œ
        "base_url": "https://api.deepseek.com"
    }
}
```

### 3. åˆ›å»ºæ•°æ®æå–æ¨¡æ¿
åˆ›å»º `templates/my_template.json`:
```json
{
  "äº§å“åç§°": "äº§å“çš„å®Œæ•´åç§°",
  "ä»·æ ¼": "äº§å“ä»·æ ¼ï¼ŒåŒ…å«è´§å¸å•ä½",
  "æè¿°": "äº§å“æè¿°æˆ–ç®€ä»‹"
}
```

### 4. è¿è¡Œçˆ¬è™«

#### æ–¹å¼1: ä¸€é”®è¿è¡Œï¼ˆæ¨èï¼‰

```python
from src.two_stage_crawler import TwoStageCrawler
import asyncio

async def main():
    crawler = TwoStageCrawler(
        task_name="my_task",              # ä»»åŠ¡åç§°ï¼ˆè¾“å‡ºç›®å½•åï¼‰
        start_url="https://example.com",  # èµ·å§‹URL
        template_path="templates/my_template.json",
        llm_config_key="deepseek"
    )

    # ä¸€é”®è¿è¡Œï¼ˆè‡ªåŠ¨æ£€æµ‹æ–­ç‚¹ç»­çˆ¬ï¼‰
    await crawler.run(
        url_patterns=["/product/", "/item/"],  # URLè¿‡æ»¤æ¨¡å¼
        stage1_max_depth=3,        # é˜¶æ®µ1æœ€å¤§æ·±åº¦
        stage1_max_pages=100,      # é˜¶æ®µ1æœ€å¤§é¡µé¢æ•°
        stage2_batch_size=10       # é˜¶æ®µ2æ‰¹æ¬¡å¤§å°ï¼ˆæ¯æ¬¡çˆ¬10ä¸ªï¼‰
    )

asyncio.run(main())
```

#### æ–¹å¼2: åˆ†æ­¥è¿è¡Œï¼ˆæ›´çµæ´»ï¼‰

```python
crawler = TwoStageCrawler(
    task_name="my_task",
    start_url="https://example.com",
    template_path="templates/my_template.json",
    llm_config_key="deepseek"
)

# é˜¶æ®µ1: æ”¶é›†é“¾æ¥ï¼ˆåªéœ€è¿è¡Œä¸€æ¬¡ï¼‰
await crawler.stage1_collect_links(
    max_depth=3,
    max_pages=100
)

# é˜¶æ®µ2: åˆ†æ‰¹çˆ¬å–ï¼ˆå¯ä»¥å¤šæ¬¡è¿è¡Œï¼Œè‡ªåŠ¨è·³è¿‡å·²çˆ¬å–çš„ï¼‰
await crawler.stage2_extract_products(
    url_patterns=["/product/"],
    batch_size=10,       # æ¯æ¬¡çˆ¬10ä¸ª
    save_interval=5      # æ¯5ä¸ªä¿å­˜ä¸€æ¬¡
)

# æŸ¥çœ‹æ‘˜è¦
crawler.print_summary()

# ä¿å­˜ç»“æœ
crawler.save_products()
```

### 5. æŸ¥çœ‹ç»“æœ

ç»“æœä¿å­˜åœ¨ `output/ä»»åŠ¡åç§°/` ç›®å½•ï¼š

**collected_links.json** - æ”¶é›†çš„æ‰€æœ‰é“¾æ¥
```json
{
  "task_name": "my_task",
  "total_links": 62,
  "crawled_count": 10,
  "links": [
    {
      "url": "https://example.com/product/1",
      "crawled": true,
      "discovered_at": "2025-11-24T16:59:30",
      "crawled_at": "2025-11-24T17:00:17"
    }
  ]
}
```

**products.json** - æå–çš„äº§å“æ•°æ®
```json
{
  "task_name": "my_task",
  "template": { ... },
  "crawl_info": {
    "products_extracted": 2,
    "last_updated": "2025-11-24T17:00:56"
  },
  "products": [
    {
      "äº§å“åç§°": "å®é™…æå–çš„åç§°",
      "ä»·æ ¼": "å®é™…æå–çš„ä»·æ ¼",
      "_source_url": "https://example.com/product/1",
      "_crawled_at": "2025-11-24T17:00:17"
    }
  ]
}
```

## ä½¿ç”¨åœºæ™¯

### åœºæ™¯1: å¤§å‹ç½‘ç«™åˆ†æ‰¹çˆ¬å–

```python
# å‡è®¾ç½‘ç«™æœ‰500ä¸ªäº§å“é¡µï¼Œåˆ†5æ‰¹å®Œæˆ

crawler = TwoStageCrawler(
    task_name="large_site",
    start_url="https://example.com",
    template_path="templates/product.json",
    llm_config_key="deepseek"
)

# ç¬¬1å¤©ï¼šæ”¶é›†æ‰€æœ‰é“¾æ¥
await crawler.stage1_collect_links(max_depth=5, max_pages=1000)

# ç¬¬2å¤©ï¼šçˆ¬å–å‰100ä¸ª
await crawler.stage2_extract_products(
    url_patterns=["/product/"],
    batch_size=100
)

# ç¬¬3-6å¤©ï¼šæ¯å¤©100ä¸ªï¼Œè‡ªåŠ¨è·³è¿‡å·²çˆ¬å–çš„
# å¤šæ¬¡è¿è¡Œï¼Œç›´åˆ°å…¨éƒ¨å®Œæˆ
```

### åœºæ™¯2: æ„å¤–ä¸­æ–­æ¢å¤

```python
# ç¨‹åºä¸­æ–­åï¼Œç›´æ¥è¿è¡Œ - è‡ªåŠ¨ä»æ–­ç‚¹æ¢å¤
await crawler.run(
    url_patterns=["/product/"],
    stage1_max_depth=3,
    stage1_max_pages=100
)

# è¾“å‡ºä¼šæ˜¾ç¤ºï¼š
# - é˜¶æ®µ1å·²å®Œæˆï¼Œè·³è¿‡
# - ä»æ–‡ä»¶åŠ è½½äº† 62 ä¸ªé“¾æ¥
# - å…¶ä¸­å·²çˆ¬å–: 15 ä¸ªï¼Œæœªçˆ¬å–: 47 ä¸ª
# - ç»§ç»­çˆ¬å–å‰©ä½™çš„47ä¸ª
```

### åœºæ™¯3: æµ‹è¯•å’Œè°ƒè¯•

```python
# å…ˆå°æ‰¹é‡æµ‹è¯•ï¼Œç¡®è®¤æ¨¡æ¿å’Œè¿‡æ»¤è§„åˆ™æ­£ç¡®

# é˜¶æ®µ1æ”¶é›†é“¾æ¥
await crawler.stage1_collect_links(max_depth=2, max_pages=50)

# å…ˆçˆ¬2ä¸ªæµ‹è¯•
await crawler.stage2_extract_products(
    url_patterns=["/product/"],
    batch_size=2
)

# æ£€æŸ¥ç»“æœï¼Œç¡®è®¤æ— è¯¯åç»§ç»­
await crawler.stage2_extract_products(
    url_patterns=["/product/"],
    batch_size=100  # æ­£å¼æ‰¹é‡çˆ¬å–
)
```

## ä¸ºä»€ä¹ˆé€‰æ‹©ä¸¤é˜¶æ®µçˆ¬å–ï¼Ÿ

| å¯¹æ¯”é¡¹ | ä¼ ç»Ÿè¾¹çˆ¬è¾¹åˆ†æ | ä¸¤é˜¶æ®µçˆ¬å– â­ |
|-------|--------------|-------------|
| LLMè°ƒç”¨æ¬¡æ•° | æ¯ä¸ªé¡µé¢1æ¬¡ | åªå¯¹äº§å“é¡µ1æ¬¡ |
| Tokenæ¶ˆè€— | é«˜ï¼ˆ100%ï¼‰ | ä½ï¼ˆçº¦20%ï¼‰ |
| è¦†ç›–èŒƒå›´ | æœ‰é™ | å…¨é¢ |
| çµæ´»æ€§ | ä½ | é«˜ |
| æ–­ç‚¹ç»­çˆ¬ | ä¸æ”¯æŒ | æ”¯æŒ |
| æ‰¹æ¬¡å¤„ç† | ä¸æ”¯æŒ | æ”¯æŒ |

**è¯¦ç»†å¯¹æ¯”è§**: [docs/STRATEGY_COMPARISON.md](docs/STRATEGY_COMPARISON.md)

## é…ç½®è¯´æ˜

### LLM é…ç½® (`configs/config.py`)
```python
LLM_CONFIG = {
    "deepseek": {
        "provider": "deepseek/deepseek-chat",
        "api_token": "your-api-key",
        "base_url": "https://api.deepseek.com"
    }
}
```

### ä»»åŠ¡é…ç½® (`configs/two_stage_tasks.json`)
```json
{
  "tasks": [
    {
      "task_id": "my_task_001",
      "task_name": "my_task",
      "start_url": "https://example.com",
      "template_path": "templates/my_template.json",
      "stage1": {
        "max_depth": 3,
        "max_pages": 100
      },
      "stage2": {
        "url_patterns": ["/product/", "/item/"],
        "batch_size": 10
      }
    }
  ]
}
```

## é‡ç½®å’Œé‡æ–°å¼€å§‹

```bash
# åˆ é™¤æ•´ä¸ªä»»åŠ¡ç›®å½•ï¼ˆå®Œå…¨é‡æ–°å¼€å§‹ï¼‰
rm -rf output/ä»»åŠ¡åç§°/

# åªåˆ é™¤é˜¶æ®µ1æ ‡å¿—ï¼ˆé‡æ–°æ”¶é›†é“¾æ¥ï¼Œä¿ç•™å·²çˆ¬å–çŠ¶æ€ï¼‰
rm output/ä»»åŠ¡åç§°/.stage1_completed

# æˆ–åœ¨ä»£ç ä¸­ä½¿ç”¨ force=True
await crawler.stage1_collect_links(force=True)
```

## æ–‡æ¡£

- **æ‰¹æ¬¡çˆ¬å–è¯¦ç»†è¯´æ˜**: [docs/BATCH_CRAWLING.md](docs/BATCH_CRAWLING.md)
- **ç­–ç•¥å¯¹æ¯”**: [docs/STRATEGY_COMPARISON.md](docs/STRATEGY_COMPARISON.md)
- **ç›®å½•ç»“æ„**: [docs/DIRECTORY.md](docs/DIRECTORY.md)

## ä¼˜åŠ¿æ€»ç»“

âœ… **æˆæœ¬ä½**: åªå¯¹äº§å“é¡µè°ƒç”¨LLMï¼ŒèŠ‚çœ80%+ token
âœ… **è¦†ç›–å…¨**: å¯ä»¥çˆ¬å–æ›´å¤šé¡µé¢ï¼Œå‘ç°æ›´å¤šäº§å“
âœ… **å®¹é”™å¼º**: æ„å¤–ä¸­æ–­åå¯ä»¥ç»§ç»­
âœ… **çµæ´»æ€§**: å¯ä»¥æ§åˆ¶æ¯æ¬¡çˆ¬å–çš„æ•°é‡
âœ… **æ˜“è°ƒè¯•**: å¯ä»¥å°æ‰¹é‡æµ‹è¯•åå†å¤§è§„æ¨¡çˆ¬å–
âœ… **æ— é‡å¤**: è‡ªåŠ¨å»é‡ï¼Œä¸ä¼šé‡å¤çˆ¬å–

## License

MIT
